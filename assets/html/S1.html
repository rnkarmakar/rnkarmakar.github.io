<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithmic Regularization: Layers are Automatically Balanced</title>
    <!-- MathJax 3 for rendering LaTeX math -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                macros: {
                  Tr: '\\operatorname{Tr}',
                  grad: '\\operatorname{grad}'
                },
                packages: {'[+]': ['physics', 'ams']}
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

    <style>
        /* Basic Reset & Body Styling */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            margin: 0;
            padding: 0;
            background-color: #f0f0f0;
            color: #333;
            line-height: 1.6;
            overflow: hidden; /* Prevent body scrollbar */
        }

        /* Presentation Container */
        .presentation-container {
            position: relative; 
            width: 100vw; 
            height: 100vh; 
            overflow: hidden;
            background-color: #f8f8f8;
        }

        /* Slide Styling */
        .slide {
            background-color: #ffffff; 
            border: 1px solid #ddd; 
            border-radius: 8px;
            padding: 40px 60px;
            margin: 0; 
            width: 100%; 
            height: 100%; 
            box-shadow: 0 4px 12px rgba(0,0,0,0.08);
            display: flex; 
            flex-direction: column; 
            justify-content: flex-start; 
            align-items: center;
            box-sizing: border-box; 
            position: absolute; 
            top: 0; 
            left: 0;
            opacity: 0; 
            visibility: hidden; 
            transition: opacity 0.8s ease-in-out, visibility 0s 0.8s;
            z-index: 1; 
            overflow-y: auto; /* Allow scrolling within a single slide if content overflows */
        }
        
        .slide.active {
            opacity: 1; 
            visibility: visible; 
            transition: opacity 0.8s ease-in-out; 
            z-index: 2;
        }

        /* Column Layout */
        .slide.columns {
            flex-direction: row; 
            justify-content: space-between; 
            align-items: stretch; /* Align items to fill height */
            gap: 30px;
        }
        
        .column-text {
            flex: 1; 
            max-width: 50%;
            padding: 20px 0;
        }
        
        .column-details {
            flex: 1.2;
            padding: 20px;
            background-color: #f9faff;
            border-left: 3px solid #0056b3;
            border-radius: 4px;
            align-self: center;
            max-height: 80vh;
            overflow-y: auto;
        }

        .column-details h4 {
            color: #0056b3;
            margin-top: 0;
            text-align: left;
            padding-bottom: 5px;
            border-bottom: 1px solid #ddd;
            width: 100%;
            position: relative;
        }
        
        .column-details h4:after { display: none; } /* Disable default h-styles */

        .column-details p { font-size: 1em; text-align: left; }
        .column-details p em { color: #333; font-weight: 500;}


        /* Headings */
        h1, h2, h3 { 
            color: #0056b3; 
            text-align: center; 
            width: 100%;
            position: relative;
            padding-bottom: 10px;
        }
        
        h1 { 
            font-size: 2.4em; 
            margin-bottom: 15px; 
            background: linear-gradient(to right, #0056b3, #0088cc);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        h2 { 
            font-size: 1.9em; 
            margin-bottom: 20px; 
        }
        
        h2:after {
            content: ''; display: block; width: 100px; height: 3px;
            background: linear-gradient(to right, #0056b3, #80cbe5);
            position: absolute; bottom: 0; left: 50%;
            transform: translateX(-50%); border-radius: 2px;
        }
        
        h3 { 
            font-size: 1.3em; color: #555; margin-top: 5px; font-weight: 500;
        }

        /* Text Elements */
        .slide p, .slide ul, .slide ol {
            margin-bottom: 18px; 
            font-size: 1.15em; 
            max-width: 900px; 
            width: 100%;
            line-height: 1.7;
            text-align: left;
        }
        
        .slide.title-slide p, .slide.centered-text p {
            text-align: center;
        }
        
        .slide ul, .slide ol { padding-left: 40px; }
        .slide ul li { list-style-type: '▸  '; }
        .slide li { margin-bottom: 10px; }
        .slide ul ul, .slide ol ol { margin-left: 30px; margin-top: 8px; }
        
        /* Special block for theorems */
        .theorem-box {
            border: 2px solid #0056b3;
            border-radius: 8px;
            padding: 20px 30px;
            margin: 20px 0;
            background-color: #f7faff;
            box-shadow: 0 2px 8px rgba(0, 86, 179, 0.1);
            width: 90%;
            max-width: 800px;
            text-align: center;
        }
        .theorem-box p { text-align: left; font-size: 1.1em; }

        /* Title Slide */
        .title-slide { 
            text-align: center;
            background: linear-gradient(to bottom, #ffffff, #f0f8ff);
        }
        .title-slide h1 { margin-top: 40px; font-size: 2.8em; }
        .title-slide h3 { margin-top: 15px; margin-bottom: 20px; color: #444; }

        /* Navigation */
        .nav-button {
            position: fixed; bottom: 25px; background-color: rgba(0, 86, 179, 0.8);
            color: white; border: none; padding: 12px 25px; font-size: 1.3em;
            border-radius: 30px; cursor: pointer; z-index: 10; 
            transition: all 0.3s ease; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }
        .nav-button:hover { background-color: rgba(0, 86, 179, 1); transform: translateY(-2px); }
        .nav-button:disabled { background-color: rgba(150, 150, 150, 0.5); cursor: not-allowed; transform: none; box-shadow: none; }
        #prevBtn { left: 25px; }
        #nextBtn { right: 25px; }

        /* Slide counter & Progress bar */
        .slide-counter {
            position: fixed; bottom: 25px; left: 50%; transform: translateX(-50%);
            background-color: rgba(0, 0, 0, 0.6); color: white; padding: 8px 15px;
            border-radius: 20px; font-size: 0.9em; z-index: 10;
        }
        .progress-bar {
            position: fixed; top: 0; left: 0; height: 5px; background-color: #0088cc;
            z-index: 20; transition: width 0.3s ease;
        }
        
        /* Media Queries */
        @media (max-width: 900px) {
            .slide { padding: 20px 25px; }
            .slide.columns { flex-direction: column; align-items: center; }
            .column-text, .column-details { max-width: 100%; flex: 1; border-left: none; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.6em; }
        }

    </style>
</head>
<body>
    <div class="presentation-container">
        <div class="progress-bar"></div>
        <div class="slide-counter"></div>

        <!-- SLIDE 1: Title -->
        <section class="slide title-slide active">
            <h1>Deep Linear Networks</h1>
            <h3>Rathindra Nath Karmakar</h3>
            <h3 style="margin-top:100px;">Session 1 : Layers are automatically balanced</h3>
        </section>

            <section class="slide">
                <h2>References</h2>
                <ul style="margin-top: 30px;">
                <li style="margin-bottom: 18px;"><b>Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced</b>, Simon S. Du, Wei Hut, Jason D. Lee (2018)</li>
                <li style="margin-bottom: 18px;"><b>The geometry of the deep linear network</b>, Govind Menon (2024)</li>
                </ul>
            </section>        

        <!-- Insert after the title slide and before "The Learning Problem" -->
        <section class="slide">
            <h2>Two Key Problems</h2>
            <ul>
                <li><strong>Supervised Learning:</strong>
                    Adjust the weights of a deep neural network so its predictions match the targets in a dataset as closely as possible.
                </li>
                <li><strong>Matrix Factorization:</strong>
                    Given a target matrix, find two smaller matrices whose product closely approximates the target. 
                </li>
            </ul>
            <p style="text-align:center; margin-top:30px;">
                <em>Both problems are solved using gradient-based optimization
             </em>
            </p>
        </section>        

        <!-- Replace the single "Learning Problem" slide with three focused slides -->
        <!-- SLIDE: Learning Problem - Goal & Model -->
        <section class="slide">
            <h2>The Learning Problem: Goal & Model</h2>
            <ol>
                <li><strong>Goal:</strong> Learn a function $f: \mathcal{X} \to \mathcal{Y}$ from a dataset of input-output pairs $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^m$.</li>
                <li><strong>Model:</strong> We use a deep neural network, parameterized by weights $\mathbf{W}$, as our function approximator, $f(x; \mathbf{W})$.</li>
            </ol>
        </section>

        <!-- SLIDE: Learning Problem - Loss Function -->
        <section class="slide">
            <h2>The Learning Problem: Loss Function</h2>
            <ol start="3">
                <li><strong>Loss:</strong> We quantify prediction error using a loss function:</li>
            </ol>
            <div class="theorem-box">
                \[ \mathcal{L}(\mathbf{W}) = \frac{1}{m} \sum_{i=1}^m L(f(x_i; \mathbf{W}), y_i) \]
            </div>
        </section>

        <!-- SLIDE: Learning Problem - Optimization -->
        <section class="slide">
            <h2>The Learning Problem: Optimization</h2>
            <ol start="4">
                <li><strong>Optimization:</strong> We find the best weights by minimizing the loss using the continuous-time dynamics of <strong>Gradient Flow</strong>.</li>
            </ol>
            <div class="theorem-box">
                \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
            </div>
        </section>

        <section class="slide" id="gradient-flow-questions">
            <h2>Gradient Flow: Key Equation</h2>
            <div class="theorem-box move-animate" id="theorem-animate">
                \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
            </div>
            <h3 style="margin-top: 60px; color: #0056b3;">Questions</h3>
            <ul style="margin-top: 20px;">
                <li>Convergence guarantees?</li>
                <li>Convergence rate?</li>
                <li>Non-convex case: Characterize the minimizer reached</li>
                <li>Effect of noise due to floating point errors, etc?</li>
            </ul>
        </section>
        
        <!-- SLIDE 3: Setup -->
        <section class="slide">
            <h2>Our Focus: Deep Homogeneous Models</h2>
            <ul>
                <li>A feed-forward neural network computes a function:
                    \[ f(x; \mathbf{W}) = W^{(N)} \phi(\cdots \phi(W^{(1)}x)\cdots) \]
                </li>
                <li>We focus on networks where the activation function $\phi$ is <strong>homogeneous</strong>.</li>
            </ul>
        </section>

        <!-- SLIDE 4: Homogeneity -->
        <section class="slide">
            <h2>A Key Property: Homogeneity</h2>
            <p>An activation function $\phi$ is homogeneous of degree 1 if for any scalar $c > 0$:</p>
            <div class="theorem-box" style="width: auto; display: inline-block; padding: 10px 30px;">
                \[ \phi(cz) = c\phi(z) \]
            </div>
            <ul>
                <li>This property holds for many popular activations:
                    <ul>
                        <li><strong>Linear:</strong> $\phi(z) = z$</li>
                        <li><strong>ReLU:</strong> $\phi(z) = \max(0, z)$</li>
                        <li><strong>Leaky ReLU:</strong> $\phi(z) = \max(az, z)$ for $0 < a < 1$</li>
                    </ul>
                </li>
                <li>It does <em>not</em> hold for functions like Sigmoid or Tanh.</li>
            </ul>
        </section>

        <!-- SLIDE 5: Scale Invariance -->
        <section class="slide">
            <h2>The Consequence: Scale Invariance</h2>
            <p>Homogeneity in activations leads to a scale invariance in the entire network. We can scale up one layer's weights and scale down the next, and the overall function remains unchanged.</p>
            <div class="theorem-box">
                 \[ f\left(x; \dots, cW^{(i)}, \frac{1}{c}W^{(i+1)}, \dots\right) = f\left(x; \dots, W^{(i)}, W^{(i+1)}, \dots\right) \]
            </div>
            <p>This is because the scalar $c$ passes through the homogeneous activation $\phi$ and cancels out.</p>
        </section>
        
        <!-- SLIDE 6: Optimization Puzzle -->
        <section class="slide">
            <h2>The Optimization Puzzle</h2>
            <p>This scale invariance creates a fundamental problem for optimization:</p>
            <ul>
                <li><strong>Non-uniqueness:</strong> If we find one good set of weights, there are infinitely many other sets that are just as good.
                    \[ \left(W^{(i)}, W^{(i+1)}\right) \text{ is equivalent to } \left(cW^{(i)}, \frac{1}{c}W^{(i+1)}\right) \]
                </li>
                <li><strong>Unboundedness:</strong> We can make the norm of the weights arbitrarily large (by choosing a large $c$) without changing the loss.
                    \[ \|cW^{(i)}\|_F \to \infty \text{ as } c \to \infty \]
                </li>
            </ul>
        </section>

        <!-- SLIDE 7: Why Classical Theory Fails -->
        <section class="slide">
            <h2>Why Classical Theory Fails</h2>
            <p>Many classical theorems that guarantee convergence of optimization algorithms rely on assumptions that are violated here.</p>
            <div class="theorem-box" style="width: 80%;">
                <p><strong>Classical Assumption:</strong> The loss function is <em>coercive</em> (i.e., $\|\mathbf{W}\| \to \infty \implies \mathcal{L}(\mathbf{W}) \to \infty$) or the parameters are constrained to a compact set.</p>
            </div>
            <ul>
                <li>In deep homogeneous models, the loss landscape has "valleys" extending to infinity where the loss is low.</li>
                <li>The parameter iterates are not guaranteed to stay in a bounded region, making convergence analysis extremely difficult.</li>
            </ul>
        </section>

        <!-- SLIDE 10: Explicit Regularization for Deep Models -->
        <section class="slide">
        <h2>The "Obvious" Fix: Explicit Regularization</h2>
        <p>For a deep network, the most common form of explicit regularization is L2 regularization, also known as weight decay:</p>
        <div class="theorem-box">
            \[
            \mathcal{L}_{reg}(\mathbf{W}) = \mathcal{L}(\mathbf{W}) + \lambda \sum_{h=1}^{N} \|W^{(h)}\|_F^2
            \]
        </div>
        <ul>
            <li>This regularizer penalizes large weights in <em>any</em> layer, discouraging norms from becoming excessively large.</li>
            <li>This makes the objective coercive (the loss grows as weights increase), ensuring the parameters remain in a bounded set where analysis is more straightforward.</li>
        </ul>
        </section>       

        <!-- SLIDE 8: Concrete Example -->
        <section class="slide">
            <h2>Matrix Factorization</h2>
            <p>The objective is to find factors $U \in \mathbb{R}^{d_1 \times r}$ and $V \in \mathbb{R}^{d_2 \times r}$ that approximate a target matrix $M^* \in \mathbb{R}^{d_1 \times d_2}$ where the rank $r \ll \min(d_1, d_2)$.</p>
            <div class="theorem-box">
                 \[ \min_{U,V} f(U,V) = \frac{1}{2} \|UV^T - M^*\|_F^2 \]
            </div>
            <ul>
                <li>This model is homogeneous: for any $c > 0$, the function is unchanged if we replace $(U, V)$ with $(cU, \frac{1}{c}V)$.</li>
                <li>The weights can become unbalanced and grow infinitely large.</li>
            </ul>
        </section>

        <!-- SLIDE 9: Explicit Regularization -->
        <section class="slide">
            <h2>The "Obvious" Fix: Explicit Regularization</h2>
            <p>A common theoretical workaround is to add a penalty term to the loss that forces the norms of the layers to be balanced:</p>
            <div class="theorem-box">
            \[ \min_{U,V} \frac{1}{2} \|UV^T - M^*\|_F^2 + \frac{\lambda}{4} \|U^T U - V^T V\|_F^2 \]
            </div>
            <ul>
                <li>This regularizer removes the homogeneity and forces $\|U\|_F \approx \|V\|_F$.</li>
                <li>The modified objective is now "well-behaved" (e.g., coercive), and standard theory can be applied to prove convergence.</li>
            </ul>
        </section>

        <!-- SLIDE 10: Empirical - Convergence -->
        <section class="slide">
            <h2>But... Is It Necessary?</h2>
            <p>Let's compare Gradient Descent on the original vs. the regularized problem. (Fig 1a from Du et al. 2018)</p>
            <img src="/assets/images/Matrix_Exp1.png" alt="Convergence plot" style="width: 60%; max-width: 500px; border: 1px solid #ccc; border-radius: 8px;">
            <p style="text-align:center;">The convergence rates are competitive. The original, unregularized problem converges just fine.</p>
        </section>

        <!-- SLIDE 11: Empirical - Surprise -->
        <section class="slide">
            <h2>The Empirical Surprise</h2>
            <p style="text-align:center;">What happens to the norms of the factors during training? (Fig 1b from Du et al. 2018)</p>
            <img src="/assets/images/Matrix_Exp2.png" alt="Norm ratio plot" style="width: 60%; max-width: 500px; border: 1px solid #ccc; border-radius: 8px;">
            <p style="text-align:center;">The ratio of the norms remains constant, even for the unregularized objective. The layers stay balanced on their own!</p>
        </section>

        <!-- SLIDE 11: Empirical - Surprise -->
        <section class="slide">
            <h2>The Empirical Surprise</h2>
            <p style="text-align:center;">Similar observation for the Deep Linear Networks (DLN) (Fig 2 from Du et al. 2018)</p>
            <img src="/assets/images/DLN_Exp1.png" alt="Norm ratio plot" style="width: 60%; max-width: 500px; border: 1px solid #ccc; border-radius: 8px;">
            <p style="text-align:center;">The ratio of the norms remains constant, even for the unregularized objective. The layers stay balanced on their own!</p>
        </section>        
        
        <!-- SLIDE 12: Central Question -->
        <section class="slide centered-text">
            <h2 style="margin-top: 15vh;">The Central Question</h2>
            <p style="font-size: 1.6em; margin-top: 50px;">Why does Gradient Descent automatically balance the layers and converge?</p>
        </section>

        <!-- SLIDE 13: The Answer 1/3 -->
        <section class="slide">
            <h2>The Answer: Implicit Regularization</h2>
            <ul>
                <li>The combination of the over-parameterized, homogeneous model structure and first-order optimization methods leads to a hidden property: <strong>Implicit Regularization</strong>.</li>
            </ul>
        </section>
        
        <!-- SLIDE 14: The Answer 2/3 -->
        <section class="slide">
            <h2>The Answer: Implicit Regularization</h2>
            <ul>
                <li>The combination of the over-parameterized, homogeneous model structure and first-order optimization methods leads to a hidden property: <strong>Implicit Regularization</strong>.</li>
                <li>The optimization dynamics on these models implicitly enforce a constraint that keeps the layer norms balanced.</li>
            </ul>
        </section>

        <!-- SLIDE 15: The Answer 3/3 -->
        <section class="slide">
            <h2>The Answer: Implicit Regularization</h2>
            <ul>
                <li>The combination of the over-parameterized, homogeneous model structure and first-order optimization methods leads to a hidden property: <strong>Implicit Regularization</strong>.</li>
                <li>The optimization dynamics on these models implicitly enforce a constraint that keeps the layer norms balanced.</li>
                <li>This prevents the parameters from running off to infinity in an unbalanced way.</li>
            </ul>
        </section>
        
        <!-- SLIDE 16: Gradient Flow -->
        <section class="slide">
            <h2>Let's Formalize: Gradient Flow</h2>
            <p>To analyze this, we study the dynamics of Gradient Descent in the continuous-time limit.</p>
            <p>The weights $\mathbf{W}$ evolve over time $t$ according to the differential equation:</p>
            <div class="theorem-box">
                \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
            </div>
            <p>This captures the behavior of GD with an infinitesimally small learning rate.</p>
        </section>

        <!-- SLIDE 17: Exact Autobalancing -->
        <section class="slide columns">
            <div class="column-text">
                <h2>Result 1: Exact Autobalancing</h2>
                <p>For any deep network with homogeneous activations, gradient flow conserves a remarkable quantity at <strong>every single neuron</strong>.</p>
                <div class="theorem-box">
                    <p><strong>Theorem 2.1 (Du, Hu, Lee 2018).</strong> <em>Under gradient flow, for any neuron $i$ in any hidden layer $h$, the difference between the squared norms of its incoming and outgoing weights is conserved:</em></p>
                    \[ \frac{d}{dt} \left( \|W^{(h)}[i,:]\|_2^2 - \|W^{(h+1)}[:,i]\|_2^2 \right) = 0 \]
                </div>
            </div>
            <div class="column-details">
                <h4>Interpretation</h4>
                <p>The squared norm of the <em>incoming</em> weights to a neuron, $\|W^{(h)}[i,:]\|_2^2$, is the vector of weights feeding into neuron $i$.</p>
                <p>The squared norm of the <em>outgoing</em> weights, $\|W^{(h+1)}[:,i]\|_2^2$, is the vector of weights coming from neuron $i$.</p>
                <p>The theorem states that the difference between these two quantities is an <strong>invariant of motion</strong>. It does not change throughout training.</p>
            </div>
        </section>

        <!-- SLIDE 18: Layer-wise Balancing -->
        <section class="slide">
            <h2>Corollary: Layer-wise Balancing</h2>
            <p>By summing over all neurons in a layer, we get a direct consequence for the entire weight matrices.</p>
            <div class="theorem-box">
                <p><strong>Corollary 2.1.</strong> <em>For any adjacent layers $h$ and $h+1$, the difference between their squared Frobenius norms is conserved under gradient flow:</em></p>
                \[ \frac{d}{dt} \left( \|W^{(h)}\|_F^2 - \|W^{(h+1)}\|_F^2 \right) = 0 \]
            </div>
        </section>

        <!-- SLIDE 19: Implication -->
        <section class="slide">
            <h2>The Implication</h2>
            <p>The difference in squared norms between adjacent layers is constant over time.</p>
            <div class="theorem-box" style="text-align:center;">
                \[ \|W^{(h)}(t)\|_F^2 - \|W^{(h+1)}(t)\|_F^2 = \text{Constant} \]
            </div>
            <ul>
                <li>This explains the empirical observation!</li>
                <li>If we initialize the network with weights that are balanced (i.e., $\|W^{(h)}(0)\|_F^2 = \|W^{(h+1)}(0)\|_F^2$), they will remain balanced for all time.</li>
                <li>Common initialization schemes like Xavier or Kaiming initialization do precisely this. They set the initial norms to be roughly equal. (More on "roughly equal" later)</li>
            </ul>
        </section>

        <!-- SLIDE 20: Stronger Invariance for Linear Networks -->
        <section class="slide">
            <h2>A Stronger Invariance for Linear Networks</h2>
            <p>For the special case of linear activations ($\phi(x)=x$), an even stronger, matrix-level quantity is conserved.</p>
            <div class="theorem-box">
                <p><strong>Theorem 2.2 (Du, Hu, Lee 2018).</strong> <em>If activation between layers $h$ and $h+1$ is linear, then under gradient flow:</em></p>
                \[ \frac{d}{dt} \left( W^{(h)}(t) (W^{(h)}(t))^T - (W^{(h+1)}(t))^T W^{(h+1)}(t) \right) = 0 \]
            </div>
            <p>This shows that not just the difference of squared norms, but a more structured matrix difference, remains constant throughout training.</p>
        </section>

        <!-- SLIDE 21: Connection to Matrix Factorization -->
        <section class="slide">
            <h2>Connecting to Matrix Factorization</h2>
            <p>The matrix factorization problem for $UV^T \approx M^*$ is equivalent to a 2-layer linear network with weights $W^{(1)} = V^T$ and $W^{(2)} = U$.</p>
            <p>Applying the stronger invariance from Theorem 2.2 to this case (with $h=1$) tells us that the quantity</p>
            \[ W^{(1)}(W^{(1)})^T - (W^{(2)})^T W^{(2)} = V^T V - U^T U \]
            <p>is conserved under gradient flow.</p>
            <p>This implies that if we initialize with $U_0^T U_0 = V_0^T V_0$, this balance will be maintained by the dynamics.</p>
        </section>

        <!-- SLIDE 22: From Theory to Practice -->
        <section class="slide">
            <h2>From Theory to Practice: Open Questions</h2>
            <p>Gradient flow with perfectly balanced initialization is an idealization. In practice, we face a more complex scenario:</p>
            <div class="theorem-box">
                \[ \mathbf{W}_{k+1} = \mathbf{W}_k - \eta \nabla \mathcal{L}(\mathbf{W}_k) \]
            </div>
            <ul>
                <li>What happens if the initialization isn't perfectly balanced?</li>
                <li>How does the discrete nature of Gradient Descent (with $\eta > 0$) affect the autobalancing property?</li>
                <li>Can we prove <strong>convergence</strong> to a global minimum for these unregularized problems?</li>
            </ul>
        </section>
        
        <!-- SLIDE 23: Approximate Balancing in GD (Rank-r) -->
        <section class="slide">
            <h2>Answers for Matrix Factorization (Rank-r)</h2>
            <p>While these questions are difficult for general deep networks, for the matrix factorization problem, we have concrete answers.</p>
            <div class="theorem-box">
                <p><strong>Lemma 3.1(i) (Du, Hu, Lee 2018).</strong> <em>For rank-r matrix factorization, assume the entries of $U_0$ and $V_0$ are initialized i.i.d. from $\mathcal{N}(0, \sigma^2)$ for a sufficiently small $\sigma^2$. If we use a decaying learning rate $\eta_t = O(t^{-(1/2+\delta)})$ for $0 < \delta \le 1/2$, then with high probability, the imbalance remains bounded for all time $t$:</em></p>
                \[ \|U_t^T U_t - V_t^T V_t\|_F < \epsilon \]
            </div>
            <p>This shows that the balancing property of gradient flow approximately holds for gradient descent.</p>
        </section>

        <!-- SLIDE 24: Stronger Results for Rank-1 Case -->
        <section class="slide">
            <h2>Stronger Results for Rank-1 Case</h2>
            <p>For the rank-1 problem, even stronger results hold, guaranteeing not just balance but also fast convergence.</p>
            <div class="theorem-box">
                <p><strong>Theorem 3.2 (Du, Hu, Lee 2018).</strong> <em>For rank-1 matrix factorization, assume $u_0 \sim \mathcal{N}(0, \delta I)$ and $v_0 \sim \mathcal{N}(0, \delta I)$ for a sufficiently small $\delta$. If we use a sufficiently small constant learning rate $\eta$, then with constant probability:</em></p>
                <ol>
                    <li style="list-style-type: '1. ';"><em>(Approximate Balancing) The ratio of the squared norms remains bounded: $0 < c_0 \le \frac{\|u_t\|_2^2}{\|v_t\|_2^2} \le C_0$.</em></li>
                    <li style="list-style-type: '2. ';"><em>(Linear Convergence) The algorithm converges to the global minimum in $O(\log(1/\epsilon))$ iterations.</em></li>
                </ol>
            </div>
            <p>This provides a comprehensive answer for the simplest non-trivial homogeneous model.</p>
        </section>

        <!-- SLIDE 25: Proof Intro -->
        <section class="slide centered-text">
            <h2>Proof Sketch</h2>
            <p style="font-size: 1.3em;">Unveiling the Mechanism of Autobalancing</p>
        </section>
        
        <!-- SLIDE 26: Proof Setup -->
        <section class="slide">
            <h2>Proof Setup: Two Linear Layers</h2>
            <p>To see the core idea, we'll prove the autobalancing result for the simplest case: a two-layer linear network.</p>
            <ul>
                <li>Consider the function $x_{out} = W_2 W_1 x_{in}$.</li>
                <li>The loss $\mathcal{L}$ is some function of the output, $L(x_{out})$.</li>
                <li>The weights evolve under gradient flow: $\frac{d}{dt}W_1 = -\nabla_{W_1}\mathcal{L}$ and $\frac{d}{dt}W_2 = -\nabla_{W_2}\mathcal{L}$.</li>
            </ul>
        </section>

        <!-- SLIDE 27: Proof Goal -->
        <section class="slide">
            <h2>The Goal</h2>
            <p>We want to prove the stronger, layer-wise balancing property that holds for linear networks (Theorem 2.2).</p>
            <div class="theorem-box">
            <p><em>Show that the following holds:</em></p>
            \[ \frac{d}{dt} \left( W_1 W_1^T - W_2^T W_2 \right) = 0 \]
            </div>
            <p>(Note: For matrices, $W W^T$ and $W^T W$ are SPSD matrices that capture the geometry of the rows and columns, respectively.)</p>
        </section>

        <!-- SLIDE 28: Proof Tool -->
        <section class="slide">
            <h2>Tool: Backpropagation (Chain Rule)</h2>
            <p>First, we write down the gradients using the chain rule (backpropagation).</p>
            <p>Let $G = \nabla_{x_{out}} L$ be the gradient with respect to the output for a single data point, and let $x_1 = W_1 x_{in}$.</p>
            <ul>
                <li>Gradient for the second layer:
                    \[ \nabla_{W_2} L = G \cdot (W_1 x_{in})^T = G \cdot x_1^T \]
                </li>
                <li style="margin-top:20px;">Gradient for the first layer:
                    \[ \nabla_{W_1} L = (\nabla_{x_1} L) \cdot x_{in}^T = (W_2^T G) \cdot x_{in}^T \]
                </li>
            </ul>
        </section>

        <!-- SLIDE 29: Proof Calc 1 -->
        <section class="slide">
            <h2>Calculation (Part 1)</h2>
            <p>Let's compute the time derivative of $W_1 W_1^T$.</p>
            \begin{align*}
            \frac{d}{dt} (W_1 W_1^T) &= \left(\frac{d W_1}{dt}\right) W_1^T + W_1 \left(\frac{d W_1}{dt}\right)^T \\[10pt]
            &= (-\nabla_{W_1} L) W_1^T - W_1 (\nabla_{W_1} L)^T \\[10pt]
            &= -(W_2^T G x_{in}^T) W_1^T - W_1 (W_2^T G x_{in}^T)^T \\[10pt]
            &= -W_2^T G (x_{in}^T W_1^T) - W_1 (x_{in} G^T W_2) \\[10pt]
            &= -W_2^T G x_1^T - W_1 x_{in} G^T W_2
            \end{align*}
        </section>
        
        <!-- SLIDE 30: Proof Calc 2 -->
        <section class="slide">
            <h2>Calculation (Part 2)</h2>
            <p>Now, let's compute the time derivative of $W_2^T W_2$.</p>
            \begin{align*}
            \frac{d}{dt} (W_2^T W_2) &= \left(\frac{d W_2}{dt}\right)^T W_2 + W_2^T \left(\frac{d W_2}{dt}\right) \\[10pt]
            &= -(\nabla_{W_2} L)^T W_2 - W_2^T (\nabla_{W_2} L) \\[10pt]
            &= -(G x_1^T)^T W_2 - W_2^T (G x_1^T) \\[10pt]
            &= -(x_1 G^T) W_2 - W_2^T G x_1^T \\[10pt]
            &= -(W_1 x_{in}) G^T W_2 - W_2^T G x_1^T
            \end{align*}
        </section>
        
        <!-- SLIDE 31: Punchline -->
        <section class="slide">
            <h2>The Punchline: Perfect Cancellation</h2>
            <p>Comparing the two results:</p>
            <p>Term 1:
            \[ \frac{d}{dt} (W_1 W_1^T) = -W_2^T G x_1^T - W_1 x_{in} G^T W_2 \]
            </p>
            <p>Term 2:
            \[ \frac{d}{dt} (W_2^T W_2) = - W_1 x_{in} G^T W_2 - W_2^T G x_1^T \]
            </p>
            <div class="theorem-box" style="margin-top:30px;">
            <p>They are exactly the same! Therefore:</p>
            \[ \frac{d}{dt} \left( W_1 W_1^T - W_2^T W_2 \right) = 0 \]
            </div>
        </section>

        <!-- SLIDE 32: Summary 1/5 -->
        <section class="slide">
            <h2>Summary of Findings</h2>
            <ul>
                <li>Deep homogeneous models have a scale invariance that makes classical optimization analysis difficult.</li>
            </ul>
        </section>
        
        <!-- SLIDE 33: Summary 2/5 -->
        <section class="slide">
            <h2>Summary of Findings</h2>
            <ul>
                <li>Deep homogeneous models have a scale invariance that makes classical optimization analysis difficult.</li>
                <li>However, first-order optimization on these over-parameterized models exhibits a powerful <strong>implicit regularization</strong>.</li>
            </ul>
        </section>

        <!-- SLIDE 34: Summary 3/5 -->
        <section class="slide">
            <h2>Summary of Findings</h2>
            <ul>
                <li>Deep homogeneous models have a scale invariance that makes classical optimization analysis difficult.</li>
                <li>However, first-order optimization on these over-parameterized models exhibits a powerful <strong>implicit regularization</strong>.</li>
                <li>It automatically balances the norms of the layers, preventing the weights from becoming pathologically unbalanced.</li>
            </ul>
        </section>

        <!-- SLIDE 35: Summary 4/5 -->
        <section class="slide">
            <h2>Summary of Findings</h2>
            <ul>
                <li>Deep homogeneous models have a scale invariance that makes classical optimization analysis difficult.</li>
                <li>However, first-order optimization on these over-parameterized models exhibits a powerful <strong>implicit regularization</strong>.</li>
                <li>It automatically balances the norms of the layers, preventing the weights from becoming pathologically unbalanced.</li>
                <li>This was shown rigorously for gradient flow (exact balancing) and then extended to practical gradient descent (approximate balancing).</li>
            </ul>
        </section>
        
        <!-- SLIDE 36: Summary 5/5 -->
        <section class="slide">
            <h2>Summary of Findings</h2>
            <ul>
                <li>Deep homogeneous models have a scale invariance that makes classical optimization analysis difficult.</li>
                <li>However, first-order optimization on these over-parameterized models exhibits a powerful <strong>implicit regularization</strong>.</li>
                <li>It automatically balances the norms of the layers, preventing the weights from becoming pathologically unbalanced.</li>
                <li>This was shown rigorously for gradient flow (exact balancing) and then extended to practical gradient descent (approximate balancing).</li>
                <li>The core mechanism lies in the symmetric structure of backpropagation.</li>
            </ul>
        </section>
        
        <!-- SLIDE 37: Divider -->
        <section class="slide centered-text">
            <h1 style="margin-top: 30vh; font-size: 3em;">Next Time...</h1>
            <p style="font-size: 1.5em;">Implicit Acceleration</p>
        </section>

        <!-- SLIDE 38: Prerequisites Intro -->
        <section class="slide">
            <h2>Prerequisites for the next day</h2>
            <ul>
                <li>Riemannian Geometry</li>
                <li>Matrix Decompositions</li>
            </ul>
        </section>

        <!-- SLIDE 39: Manifold & Tangent Space -->
        <section class="slide">
            <div>
                <h2>Riemannian Geometry</h2>
                <ul>
                    <li>A smooth <strong>Manifold</strong> $M$ is a space that is locally diffeomorphic to a Euclidean space (e.g., $\mathbb{R}^n$).</li>
                    <li>At each point $p \in M$, the <strong>Tangent Space</strong> $T_p M$ is the vector space of all velocity vectors of smooth curves on $M$ passing through $p$.</li>
                </ul>
                <p><strong>Example:</strong></p>
                <ul>
                    <li>Manifold: $M = M_{d\times n}(\mathbb{R})$, the space of matrices.</li>
                    <li>Tangent Space: For any $W \in M$, the tangent space is just a copy of the space of matrices itself: $T_W M \cong M_{d\times n}(\mathbb{R})$.</li>
                </ul>
            </div>
        </section>
        
        <!-- SLIDE 40: Metric & Gradient -->
        <section class="slide">
            <h2> The Riemannian Metric & Gradient</h2>
            <ul>
                <li>A <strong>Riemannian Metric</strong> $g$ provides an inner product $g_p(\cdot, \cdot) = \langle \cdot, \cdot \rangle_p$ on each tangent space $T_p M$. This endows each tangent space with a Hilbert space structure.</li>
            </ul>
            <div class="theorem-box">
                <p><strong>Definition: The Riemannian Gradient.</strong> Given a smooth function $E: M \to \mathbb{R}$, its gradient $\grad_p E$ at a point $p$ is the unique vector in $T_p M$ that represents the derivative $dE_p$ via the metric:</p>
                \[ g_p(\grad_p E, v) = dE_p(v) \quad \text{for all } v \in T_p M \]
            </div>
             <p>The existence and uniqueness of $\grad_p E$ are guaranteed by the Riesz Representation Theorem on the Hilbert space $(T_p M, g_p)$.</p>
        </section>

        <!-- SLIDE 41: Example 1 Rigorous -->
        <section class="slide">
            <h2>Gradient Example 1: The Space of Matrices</h2>
            
            <ul>
                <li>Consider the vectorization map $\text{vec}: M_{d \times n}(\mathbb{R}) \to \mathbb{R}^{dn}$ and its inverse $\text{mat}$. </li>
                <li>The manifold $(M_{d \times n}, g_F)$ with the Frobenius metric $g_W(Z_1, Z_2) = \text{Tr}(Z_1^T Z_2)$ is <strong>isomorphic</strong> to $(\mathbb{R}^{dn}, g_E)$ with the standard Euclidean dot product $g_w(z_1, z_2) = z_1^T z_2$.</li>
            </ul>
            <div class="theorem-box">
                <p>Use the identity: $\text{Tr}(Z_1^T Z_2) = (\text{vec}(Z_1))^T (\text{vec}(Z_2))$.</p>
            </div>
            <ul>
                <li>Let $E: M_{d \times n} \to \mathbb{R}$ be our loss. Define $E_v: \mathbb{R}^{dn} \to \mathbb{R}$ as $E_v(w) = E(\text{mat}(w))$.</li>
                <li>Using the isomorphisms and chain rule, the Riemannian gradient of $E$ is simply the standard gradient of $E_v$, mapped back to the matrix space.</li>
            </ul>
            <div class="theorem-box">
                \[ \grad E = \text{mat}(\nabla E_v) \]
            </div>
        </section>

        <!-- SLIDE 42: Example 2 -->
        <section class="slide">
            <h2>Gradient Example 2: A Preconditioned Metric</h2>
            <p>Now, let's change the geometry. Let $A$ be a fixed, symmetric positive-definite (SPD) operator.</p>
            <ul>
                <li><strong>New Metric:</strong> Define a new inner product $g_W^A(Z_1, Z_2) = \text{Tr}(Z_1^T A^{-1} Z_2)$. (This is a valid inner product because $A^{-1}$ is also SPD).</li>
            </ul>
            <p>What is the Riemannian gradient, $\grad_W^A E$, under this new metric?</p>
             <div class="theorem-box">
                <p>We must again satisfy the duality condition, but now with the new metric:</p>
                \[ g_W^A(\grad_W^A E, Z) = dE_W(Z) \]
            </div>
        </section>
        
        <!-- SLIDE 43: Derivation -->
        <section class="slide">
            <h2>Deriving the Preconditioned Gradient</h2>
            <p>Let's solve for the new gradient, $\grad_W^A E$. Let $\nabla E(W)$ be the standard Euclidean gradient matrix.</p>
            <ol>
                <li>Start with the defining property:
                    \[ g_W^A(\grad_W^A E, Z) = dE_W(Z) \]
                </li>
                <li>Substitute the definitions for the metric and the directional derivative:
                    \[ \text{Tr}((\grad_W^A E)^T A^{-1} Z) = \text{Tr}((\nabla E(W))^T Z) \]
                </li>
                 <li>Using the cyclic property of the trace, $\text{Tr}(XYZ) = \text{Tr}(ZXY)$, on the left side:
                    \[ \text{Tr}(Z (\grad_W^A E)^T A^{-1}) = \text{Tr}(Z (\nabla E(W))^T) \]
                </li>
                <li>Since this must hold for all tangent vectors $Z$, the terms multiplying $Z$ must be equal:
                    \[ (\grad_W^A E)^T A^{-1} = (\nabla E(W))^T \implies A^{-1} \grad_W^A E = \nabla E(W) \]
                </li>
            </ol>
            <div class="theorem-box">
                \[ \grad_W^A E = A \nabla E(W) \]
            </div>
        </section>
        
        <!-- SLIDE 44: Takeaway -->
        <section class="slide">
            <h2>The Takeaway</h2>
            <p>The gradient of a function is not absolute; <b>it depends on the chosen geometry (metric)</b>.</p>
            <p>By changing the metric from the standard Euclidean one to a "preconditioned" one, the gradient itself is transformed:</p>
            <ul>
                <li>Frobenius Metric $g(Z_1,Z_2) = \text{Tr}(Z_1^T Z_2) \quad \implies \quad \grad_W E = \nabla E(W)$</li>
                <li>Preconditioned Metric $g^A(Z_1,Z_2) = \text{Tr}(Z_1^T A^{-1}Z_2) \quad \implies \quad \grad_W^A E = A \nabla E(W)$</li>
            </ul>
        </section>
                <!-- SLIDE 45: Prerequisites 2 Intro -->
        <section class="slide">
            <h2> Matrix Decompositions</h2>
            <p>We will also need two fundamental tools from linear algebra: the Singular Value Decomposition (SVD) and the Polar Decomposition.</p>
        </section>

        <!-- SLIDE 46: SVD -->
        <section class="slide">
            <div>
                <h2>Singular Value Decomposition (SVD)</h2>
                <div class="theorem-box">
                    <p><strong>Theorem.</strong> For any real matrix $M \in \mathbb{R}^{m \times n}$, there exists a decomposition:</p>
                    \[ M = U \Sigma V^T \]
                </div>
                <ul>
                    <li>$U \in \mathbb{R}^{m \times m}$ is an <strong>orthogonal</strong> matrix whose columns are the <em>left singular vectors</em>.</li>
                    <li>$V \in \mathbb{R}^{n \times n}$ is an <strong>orthogonal</strong> matrix whose columns are the <em>right singular vectors</em>.</li>
                    <li>$\Sigma \in \mathbb{R}^{m \times n}$ is a rectangular diagonal matrix. Its diagonal entries $\sigma_i$ are the non-negative, ordered <em>singular values</em>.</li>
                </ul>
            </div>
        </section>

        <!-- SLIDE 47: Polar Decompositions -->
        <section class="slide">
            <h2>Polar Decompositions</h2>
            <p>Analogous to a complex number $z = r e^{i\theta}$, any square matrix can be decomposed into a "stretch" and a "rotation."</p>
            <p>Any square matrix $W \in M_d$ admits both left and right polar decompositions.</p>
            <ul>
                <li><strong>Left Polar Decomposition:</strong> $W = QP$</li>
                <li><strong>Right Polar Decomposition:</strong> $W = RU^T$</li>
            </ul>
            <p>Where:</p>
            <ul>
                <li>$P = \sqrt{W^T W}$ and $R = \sqrt{W W^T}$ are unique symmetric positive semi-definite (SPSD) matrices, representing the "stretch".</li>
                <li>$Q$ and $U$ are orthogonal matrices ($Q, U \in O_d$), representing rotations/reflections.</li>
            </ul>
        </section>
        
        <!-- SLIDE 48: Connection -->
        <section class="slide">
            <h2>SVD & Polar Decompositions: The Connection</h2>
            <p>The two decompositions are intrinsically linked. Each can be derived from the other.</p>
            <p>Let the SVD of $W$ be $W = U \Sigma V^T$, where $U, V \in O_d$.</p>
            <div class="theorem-box">
                <p><strong>SVD $\implies$ Polar:</strong> We can construct the polar factors directly:</p>
                <ul>
                    <li style="list-style-type:none;">Stretch matrices: $P = V \Sigma V^T$ and $R = U \Sigma U^T$.</li>
                    <li style="list-style-type:none;">Orthogonal matrices: $Q = U V^T$ and $U = V U^T$.</li>
                </ul>
            </div>
            <div class="theorem-box">
                <p><strong>Polar $\implies$ SVD:</strong> Start with the left polar form $W = QP$.</p>
                <ol style="text-align:left;">
                    <li>Eigendecompose the SPSD matrix $P$: $P = V \Sigma V^T$.</li>
                    <li>Substitute into the polar form: $W = Q (V \Sigma V^T)$.</li>
                    <li>Group the orthogonal matrices: $W = (Q V) \Sigma V^T$.</li>
                    <li>This is the SVD of $W$, where the left singular vector matrix is $U = Q V$.</li>
                </ol>
            </div>
        </section>
        
        <!-- SLIDE 49: Q&A -->
        <section class="slide centered-text">
            <h2 style="margin-top: 15vh;">Thank You!</h2>
            <p style="font-size: 1.6em; margin-top: 50px;">Any Questions?</p>
        </section>
   

    </div><!-- end presentation-container -->

    <button id="prevBtn" class="nav-button">Previous (←)</button>
    <button id="nextBtn" class="nav-button">Next (→)</button>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const slides = document.querySelectorAll('.presentation-container .slide');
            const prevBtn = document.getElementById('prevBtn');
            const nextBtn = document.getElementById('nextBtn');
            const slideCounter = document.querySelector('.slide-counter');
            const progressBar = document.querySelector('.progress-bar');
            let currentSlideIndex = 0;

            function updateProgressBar() {
                const progress = ((currentSlideIndex + 1) / slides.length) * 100;
                progressBar.style.width = `${progress}%`;
            }

            function updateSlideCounter() {
                if (slideCounter) {
                    slideCounter.textContent = `${currentSlideIndex + 1} / ${slides.length}`;
                }
            }

            function showSlide(index) {
                if (index >= slides.length || index < 0) return;
                
                slides.forEach(slide => slide.classList.remove('active'));
                
                currentSlideIndex = index;
                
                slides[currentSlideIndex].classList.add('active');
                slides[currentSlideIndex].scrollTop = 0; // Reset scroll for new slide
                
                prevBtn.disabled = currentSlideIndex === 0;
                nextBtn.disabled = currentSlideIndex === slides.length - 1;
                
                updateSlideCounter();
                updateProgressBar();

                // Re-render MathJax for the new active slide if necessary
                if (window.MathJax) {
                    MathJax.typesetPromise([slides[currentSlideIndex]]).catch(function (err) {
                        console.log('MathJax reproccessing error:', err.message);
                    });
                }
            }

            nextBtn.addEventListener('click', () => showSlide(currentSlideIndex + 1));
            prevBtn.addEventListener('click', () => showSlide(currentSlideIndex - 1));
            
            document.addEventListener('keydown', (event) => {
                const activeElementTag = document.activeElement ? document.activeElement.tagName.toLowerCase() : null;
                if (['input', 'textarea'].includes(activeElementTag)) return;

                if (event.key === 'ArrowRight' || event.key === ' ' || event.key === 'PageDown') {
                    if (event.key === ' ') event.preventDefault();
                    if (!nextBtn.disabled) showSlide(currentSlideIndex + 1);
                } else if (event.key === 'ArrowLeft' || event.key === 'PageUp') {
                    if (!prevBtn.disabled) showSlide(currentSlideIndex - 1);
                }
            });
            
            showSlide(0);
        });
    </script>
</body>
</html>
