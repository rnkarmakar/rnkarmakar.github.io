<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Algorithmic Regularization in Deep Linear Networks (PDF Style)</title>
    <!-- MathJax 3 for rendering LaTeX math -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                macros: {
                  Tr: '\\operatorname{Tr}',
                  grad: '\\operatorname{grad}'
                },
                packages: {'[+]': ['physics', 'ams']}
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            margin: 0 auto;
            padding: 0 0 40px 0;
            background-color: #f0f0f0;
            color: #333;
            line-height: 1.7;
            max-width: 900px;
        }
        .pdf-container {
            background: #fff;
            margin: 40px auto 0 auto;
            padding: 40px 60px;
            border-radius: 10px;
            box-shadow: 0 4px 24px rgba(0,0,0,0.10);
        }
        h1, h2, h3, h4 {
            color: #0056b3;
            margin-top: 2.2em;
            margin-bottom: 0.7em;
            font-weight: 600;
        }
        h1 { font-size: 2.3em; margin-top: 0.5em; text-align: center; }
        h2 { font-size: 1.6em; border-bottom: 2px solid #a0aec0; padding-bottom: 0.2em; }
        h3 { font-size: 1.2em; color: #444; }
        h4 { font-size: 1.1em; color: #0056b3; }
        p, ul, ol {
            font-size: 1.13em;
            margin-bottom: 1.2em;
        }
        ul, ol { padding-left: 2em; }
        ul li { list-style-type: disc; margin-bottom: 8px; }
        ul ul li { list-style-type: circle; }
        .theorem-box {
            border: 2px solid #0056b3;
            border-radius: 8px;
            padding: 20px 30px;
            margin: 30px 0;
            background-color: #f7faff;
            box-shadow: 0 2px 8px rgba(0, 86, 179, 0.08);
        }
        .theorem-box p { font-size: 1.1em; font-weight: 500; }
        .columns {
            display: flex;
            gap: 40px;
            flex-wrap: wrap;
        }
        .column-text, .column-details {
            flex: 1 1 350px;
            min-width: 280px;
        }
        .column-details {
            background: #f9faff;
            border-left: 3px solid #0056b3;
            border-radius: 4px;
            padding: 18px 22px;
            padding-top: 0.5em;
        }
        @media (max-width: 900px) {
            .pdf-container { padding: 18px 8px; }
            .columns { flex-direction: column; gap: 0; }
        }
        .columns {
            display: flex;
            align-items: flex-start;
            gap: 40px;
            flex-wrap: wrap;
        }
    </style>
</head>
<body>
<div class="pdf-container">

    <h1>Deep Linear Networks</h1>
    <h3 style="text-align:center; color:#444; margin-top:-1.2em;">Rathindra Nath Karmakar</h3>
    <h3 style="text-align:center;">Session 1: Layers are automatically balanced</h3>

    <h2>References</h2>
    <ul>
        <li><b>Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced</b>, Simon S. Du, Wei Hu, Jason D. Lee (2018)</li>
        <li><b>The geometry of the deep linear network</b>, Govind Menon (2024)</li>
    </ul>

    <h2>Two Key Problems</h2>
    <ul>
        <li><strong>Supervised Learning:</strong>
            Adjust the weights of a deep neural network so its predictions match the targets in a dataset as closely as possible.
        </li>
        <li><strong>Matrix Factorization:</strong>
            Given a target matrix, find two smaller matrices whose product closely approximates the target. 
        </li>
    </ul>
    <p style="text-align:center; margin-top:30px;">
        <em>Both problems are solved using gradient-based optimization.</em>
    </p>

    <h2>The Learning Problem</h2>
    <h3>1. Goal & Model</h3>
    <p>The goal is to learn a function $f: \mathcal{X} \to \mathcal{Y}$ from a dataset of input-output pairs $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^m$. We use a deep neural network, parameterized by weights $\mathbf{W}$, as our function approximator, $f(x; \mathbf{W})$.</p>

    <h3>2. Loss Function</h3>
    <p>We quantify prediction error using a loss function:</p>
    <div class="theorem-box">
        \[ \mathcal{L}(\mathbf{W}) = \frac{1}{m} \sum_{i=1}^m L(f(x_i; \mathbf{W}), y_i) \]
    </div>

    <h3>3. Optimization</h3>
    <p>We find the best weights by minimizing the loss using the continuous-time dynamics of <strong>Gradient Flow</strong>.</p>
    <div class="theorem-box">
        \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
    </div>
    
    <h2>Gradient Flow: Key Questions</h2>
    <ul>
        <li>Convergence guarantees?</li>
        <li>Convergence rate?</li>
        <li>Non-convex case: Characterize the minimizer reached.</li>
        <li>Effect of noise due to floating point errors, etc?</li>
    </ul>

    <h2>Our Focus: Deep Homogeneous Models</h2>
    <ul>
        <li>A feed-forward neural network computes a function:
            \[ f(x; \mathbf{W}) = W^{(N)} \phi(\cdots \phi(W^{(1)}x)\cdots) \]
        </li>
        <li>We focus on networks where the activation function $\phi$ is <strong>homogeneous</strong>.</li>
    </ul>

    <h2>A Key Property: Homogeneity</h2>
    <p>An activation function $\phi$ is homogeneous of degree 1 if for any scalar $c > 0$:</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \phi(cz) = c\phi(z) \]
    </div>
    <ul>
        <li>This property holds for many popular activations:
            <ul>
                <li><strong>Linear:</strong> $\phi(z) = z$</li>
                <li><strong>ReLU:</strong> $\phi(z) = \max(0, z)$</li>
                <li><strong>Leaky ReLU:</strong> $\phi(z) = \max(az, z)$ for $0 < a < 1$</li>
            </ul>
        </li>
        <li>It does <em>not</em> hold for functions like Sigmoid or Tanh.</li>
    </ul>

    <h2>The Consequence: Scale Invariance</h2>
    <p>Homogeneity in activations leads to a scale invariance in the entire network. We can scale up one layer's weights and scale down the next, and the overall function remains unchanged.</p>
    <div class="theorem-box" style="text-align:center;">
         \[ f\left(x; \dots, cW^{(i)}, \frac{1}{c}W^{(i+1)}, \dots\right) = f\left(x; \dots, W^{(i)}, W^{(i+1)}, \dots\right) \]
    </div>
    <p>This is because the scalar $c$ passes through the homogeneous activation $\phi$ and cancels out.</p>

    <h2>The Optimization Puzzle</h2>
    <p>This scale invariance creates a fundamental problem for optimization:</p>
    <ul>
        <li><strong>Non-uniqueness:</strong> If we find one good set of weights, there are infinitely many other sets that are just as good.
            \[ \left(W^{(i)}, W^{(i+1)}\right) \text{ is equivalent to } \left(cW^{(i)}, \frac{1}{c}W^{(i+1)}\right) \]
        </li>
        <li><strong>Unboundedness:</strong> We can make the norm of the weights arbitrarily large (by choosing a large $c$) without changing the loss.
            \[ \|cW^{(i)}\|_F \to \infty \text{ as } c \to \infty \]
        </li>
    </ul>

    <h2>Why Classical Theory Fails</h2>
    <p>Many classical theorems that guarantee convergence of optimization algorithms rely on assumptions that are violated here.</p>
    <div class="theorem-box">
        <p><strong>Classical Assumption:</strong> The loss function is <em>coercive</em> (i.e., $\|\mathbf{W}\| \to \infty \implies \mathcal{L}(\mathbf{W}) \to \infty$) or the parameters are constrained to a compact set.</p>
    </div>
    <ul>
        <li>In deep homogeneous models, the loss landscape has "valleys" extending to infinity where the loss is low.</li>
        <li>The parameter iterates are not guaranteed to stay in a bounded region, making convergence analysis extremely difficult.</li>
    </ul>

    <h2>The "Obvious" Fix: Regularization for General Models</h2>
    <p>For a deep network, the most common form of explicit regularization is L2 regularization, also known as weight decay:</p>
    <div class="theorem-box" style="text-align:center;">
        \[
        \mathcal{L}_{reg}(\mathbf{W}) = \mathcal{L}(\mathbf{W}) + \lambda \sum_{h=1}^{N} \|W^{(h)}\|_F^2
        \]
    </div>
    <ul>
        <li>This regularizer penalizes large weights in <em>any</em> layer, discouraging norms from becoming excessively large.</li>
        <li>This makes the objective coercive (the loss grows as weights increase), ensuring the parameters remain in a bounded set where analysis is more straightforward.</li>
    </ul>

    <h2>Example: Matrix Factorization</h2>
    <p>The objective is to find factors $U \in \mathbb{R}^{d_1 \times r}$ and $V \in \mathbb{R}^{d_2 \times r}$ that approximate a target matrix $M^* \in \mathbb{R}^{d_1 \times d_2}$ where the rank $r \ll \min(d_1, d_2)$.</p>
    <div class="theorem-box" style="text-align:center;">
         \[ \min_{U,V} f(U,V) = \frac{1}{2} \|UV^T - M^*\|_F^2 \]
    </div>
    <ul>
        <li>This model is homogeneous: for any $c > 0$, the function is unchanged if we replace $(U, V)$ with $(cU, \frac{1}{c}V)$.</li>
        <li>The weights can become unbalanced and grow infinitely large.</li>
    </ul>

    <h2>The "Obvious" Fix: Regularization for Matrix Factorization</h2>
    <p>A common theoretical workaround is to add a penalty term to the loss that forces the norms of the layers to be balanced:</p>
    <div class="theorem-box" style="text-align:center;">
    \[ \min_{U,V} \frac{1}{2} \|UV^T - M^*\|_F^2 + \frac{\lambda}{4} \|U^T U - V^T V\|_F^2 \]
    </div>
    <ul>
        <li>This regularizer removes the homogeneity and forces $\|U\|_F \approx \|V\|_F$.</li>
        <li>The modified objective is now "well-behaved" (e.g., coercive), and standard theory can be applied to prove convergence.</li>
    </ul>
    
    <h2>But... Is It Necessary? The Empirical Surprise</h2>
    <p>Let's compare Gradient Descent on the original vs. the regularized problem for matrix factorization. (Fig 1 from Du et al. 2018)</p>
    <div style="text-align:center;">
        <img src="C:/Users/Rathindra/Documents/Menon/Matrix_Exp1.png" alt="Convergence plot" style="width: 50%; border: 1px solid #ccc; border-radius: 8px; margin-bottom: 1em;">
        <img src="C:/Users/Rathindra/Documents/Menon/Matrix_Exp2.png" alt="Norm ratio plot" style="width: 50%; border: 1px solid #ccc; border-radius: 8px; margin-bottom: 1em;">
    </div>
    <p>The unregularized problem converges just as well, and surprisingly, the ratio of the factor norms remains constant. The layers stay balanced on their own!</p>
    <p>A similar observation holds for Deep Linear Networks (DLN). (Fig 2 from Du et al. 2018)</p>
    <div style="text-align:center;">
        <img src="C:/Users/Rathindra/Documents/Menon/DLN_Exp1.png" alt="Norm ratio plot for DLN" style="width: 60%; border: 1px solid #ccc; border-radius: 8px; margin-bottom: 1em;">
    </div>

    <h2 style="text-align:center; border-bottom: none;">The Central Question</h2>
    <p style="font-size: 1.3em; text-align:center; margin-top: 1em;">Why does Gradient Descent automatically balance the layers and converge?</p>

    <h2>The Answer: Implicit Regularization</h2>
    <ul>
        <li>The combination of the over-parameterized, homogeneous model structure and first-order optimization methods leads to a hidden property: <strong>Implicit Regularization</strong>.</li>
        <li>The optimization dynamics on these models implicitly enforce a constraint that keeps the layer norms balanced.</li>
        <li>This prevents the parameters from running off to infinity in an unbalanced way.</li>
    </ul>
    
    <h2>Formalizing the Dynamics: Gradient Flow</h2>
    <p>To analyze this, we study the dynamics of Gradient Descent in the continuous-time limit. The weights $\mathbf{W}$ evolve over time $t$ according to the differential equation:</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
    </div>
    <p>This captures the behavior of GD with an infinitesimally small learning rate.</p>

    <div class="columns">
        <div class="column-text">
            <h2>Result 1: Exact Autobalancing</h2>
            <p>For any deep network with homogeneous activations, gradient flow conserves a remarkable quantity at <strong>every single neuron</strong>.</p>
            <div class="theorem-box">
                <p><strong>Theorem 2.1 (Du, Hu, Lee 2018).</strong> <em>Under gradient flow, for any neuron $i$ in any hidden layer $h$, the difference between the squared norms of its incoming and outgoing weights is conserved:</em></p>
                \[ \frac{d}{dt} \left( \|W^{(h)}[i,:]\|_2^2 - \|W^{(h+1)}[:,i]\|_2^2 \right) = 0 \]
            </div>
        </div>
        <div class="column-details">
            <h4>Interpretation</h4>
            <p>The squared norm of the <em>incoming</em> weights to a neuron, $\|W^{(h)}[i,:]\|_2^2$, is the vector of weights feeding into neuron $i$.</p>
            <p>The squared norm of the <em>outgoing</em> weights, $\|W^{(h+1)}[:,i]\|_2^2$, is the vector of weights coming from neuron $i$.</p>
            <p>The theorem states that the difference between these two quantities is an <strong>invariant of motion</strong>. It does not change throughout training.</p>
        </div>
    </div>
    
    <h2>Corollary: Layer-wise Balancing</h2>
    <p>By summing over all neurons in a layer, we get a direct consequence for the entire weight matrices.</p>
    <div class="theorem-box">
        <p><strong>Corollary 2.1.</strong> <em>For any adjacent layers $h$ and $h+1$, the difference between their squared Frobenius norms is conserved under gradient flow:</em></p>
        \[ \frac{d}{dt} \left( \|W^{(h)}\|_F^2 - \|W^{(h+1)}\|_F^2 \right) = 0 \]
    </div>

    <h2>The Implication</h2>
    <p>The difference in squared norms between adjacent layers is constant over time.</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \|W^{(h)}(t)\|_F^2 - \|W^{(h+1)}(t)\|_F^2 = \text{Constant} \]
    </div>
    <ul>
        <li>This explains the empirical observation!</li>
        <li>If we initialize the network with weights that are balanced (i.e., $\|W^{(h)}(0)\|_F^2 = \|W^{(h+1)}(0)\|_F^2$), they will remain balanced for all time.</li>
        <li>Common initialization schemes like Xavier or Kaiming initialization do precisely this. They set the initial norms to be roughly equal.</li>
    </ul>

    <h2>A Stronger Invariance for Linear Networks</h2>
    <p>For the special case of linear activations ($\phi(x)=x$), an even stronger, matrix-level quantity is conserved.</p>
    <div class="theorem-box">
        <p><strong>Theorem 2.2 (Du, Hu, Lee 2018).</strong> <em>If activation between layers $h$ and $h+1$ is linear, then under gradient flow:</em></p>
        \[ \frac{d}{dt} \left( W^{(h)}(t) (W^{(h)}(t))^T - (W^{(h+1)}(t))^T W^{(h+1)}(t) \right) = 0 \]
    </div>
    <p>This shows that not just the difference of squared norms, but a more structured matrix difference, remains constant throughout training.</p>

    <h2>Connecting to Matrix Factorization</h2>
    <p>The matrix factorization problem for $UV^T \approx M^*$ is equivalent to a 2-layer linear network with weights $W^{(1)} = V^T$ and $W^{(2)} = U$. Applying the stronger invariance from Theorem 2.2 to this case (with $h=1$) tells us that the quantity</p>
    \[ W^{(1)}(W^{(1)})^T - (W^{(2)})^T W^{(2)} = V^T V - U^T U \]
    <p>is conserved under gradient flow. This implies that if we initialize with $U_0^T U_0 = V_0^T V_0$, this balance will be maintained by the dynamics.</p>

    <h2>From Theory to Practice: Open Questions</h2>
    <p>Gradient flow with perfectly balanced initialization is an idealization. In practice, we use Gradient Descent with a finite learning rate $\eta$:</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \mathbf{W}_{k+1} = \mathbf{W}_k - \eta \nabla \mathcal{L}(\mathbf{W}_k) \]
    </div>
    <ul>
        <li>What happens if the initialization isn't perfectly balanced?</li>
        <li>How does the discrete nature of Gradient Descent affect the autobalancing property?</li>
        <li>Can we prove <strong>convergence</strong> to a global minimum for these unregularized problems?</li>
    </ul>

    <h2>Answers for Matrix Factorization</h2>
    <p>While these questions are difficult for general deep networks, for the matrix factorization problem, we have concrete answers.</p>
    <div class="theorem-box">
        <p><strong>Lemma 3.1(i) (Du, Hu, Lee 2018).</strong> <em>For rank-r matrix factorization, assume the entries of $U_0$ and $V_0$ are initialized i.i.d. from $\mathcal{N}(0, \sigma^2)$ for a sufficiently small $\sigma^2$. If we use a decaying learning rate $\eta_t = O(t^{-(1/2+\delta)})$ for $0 < \delta \le 1/2$, then with high probability, the imbalance remains bounded for all time $t$:</em></p>
        \[ \|U_t^T U_t - V_t^T V_t\|_F < \epsilon \]
    </div>
    <p>For the rank-1 case, even stronger results hold, guaranteeing not just balance but also fast convergence.</p>
    <div class="theorem-box">
        <p><strong>Theorem 3.2 (Du, Hu, Lee 2018).</strong> <em>For rank-1 matrix factorization, assume $u_0 \sim \mathcal{N}(0, \delta I)$ and $v_0 \sim \mathcal{N}(0, \delta I)$ for a sufficiently small $\delta$. If we use a sufficiently small constant learning rate $\eta$, then with constant probability:</em></p>
        <ol>
            <li><em>(Approximate Balancing) The ratio of the squared norms remains bounded: $0 < c_0 \le \frac{\|u_t\|_2^2}{\|v_t\|_2^2} \le C_0$.</em></li>
            <li><em>(Linear Convergence) The algorithm converges to the global minimum in $O(\log(1/\epsilon))$ iterations.</em></li>
        </ol>
    </div>

    <h2>Proof Sketch: Unveiling the Mechanism of Autobalancing</h2>
    <p>To see the core idea, we'll prove the autobalancing result for the simplest case: a two-layer linear network where $x_{out} = W_2 W_1 x_{in}$. The goal is to show:</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \frac{d}{dt} \left( W_1 W_1^T - W_2^T W_2 \right) = 0 \]
    </div>
    <h3>Tool: Backpropagation (Chain Rule)</h3>
    <p>Let $G = \nabla_{x_{out}} L$ be the gradient with respect to the output for a single data point, and let $x_1 = W_1 x_{in}$.</p>
    <ul>
        <li>Gradient for the second layer: $ \nabla_{W_2} L = G \cdot x_1^T $</li>
        <li>Gradient for the first layer: $ \nabla_{W_1} L = (W_2^T G) \cdot x_{in}^T $</li>
    </ul>
    <h3>Calculations</h3>
    <p>First, we compute the time derivative of $W_1 W_1^T$:</p>
    \begin{align*}
    \frac{d}{dt} (W_1 W_1^T) &= \left(\frac{d W_1}{dt}\right) W_1^T + W_1 \left(\frac{d W_1}{dt}\right)^T \\
    &= -(\nabla_{W_1} L) W_1^T - W_1 (\nabla_{W_1} L)^T \\
    &= -(W_2^T G x_{in}^T) W_1^T - W_1 (x_{in} G^T W_2) \\
    &= -W_2^T G x_1^T - W_1 x_{in} G^T W_2
    \end{align*}
    <p>Next, we compute the time derivative of $W_2^T W_2$:</p>
    \begin{align*}
    \frac{d}{dt} (W_2^T W_2) &= \left(\frac{d W_2}{dt}\right)^T W_2 + W_2^T \left(\frac{d W_2}{dt}\right) \\
    &= -(\nabla_{W_2} L)^T W_2 - W_2^T (\nabla_{W_2} L) \\
    &= -(x_1 G^T) W_2 - W_2^T G x_1^T \\
    &= -(W_1 x_{in}) G^T W_2 - W_2^T G x_1^T
    \end{align*}
    <h3>The Punchline: Perfect Cancellation</h3>
    <p>The two derivatives are exactly the same. Therefore, their difference is zero, and the quantity $W_1 W_1^T - W_2^T W_2$ is conserved.</p>
    
    <h2>Summary of Findings</h2>
    <ul>
        <li>Deep homogeneous models have a scale invariance that makes classical optimization analysis difficult.</li>
        <li>However, first-order optimization on these over-parameterized models exhibits a powerful <strong>implicit regularization</strong>.</li>
        <li>It automatically balances the norms of the layers, preventing the weights from becoming pathologically unbalanced.</li>
        <li>This was shown rigorously for gradient flow (exact balancing) and then extended to practical gradient descent (approximate balancing).</li>
        <li>The core mechanism lies in the symmetric structure of backpropagation.</li>
    </ul>

    <h1>Prerequisites for Next Session</h1>
    
    <h2>Riemannian Geometry</h2>
    <ul>
        <li>A smooth <strong>Manifold</strong> $M$ is a space that is locally diffeomorphic to a Euclidean space (e.g., $\mathbb{R}^n$).</li>
        <li>At each point $p \in M$, the <strong>Tangent Space</strong> $T_p M$ is the vector space of all velocity vectors of smooth curves on $M$ passing through $p$.</li>
    </ul>
    <p><strong>Example:</strong> For the manifold $M = M_{d\times n}(\mathbb{R})$, the space of matrices, the tangent space at any $W \in M$ is just a copy of the space of matrices itself: $T_W M \cong M_{d\times n}(\mathbb{R})$.</p>

    <h2>The Riemannian Metric & Gradient</h2>
    <ul>
        <li>A <strong>Riemannian Metric</strong> $g$ provides an inner product $g_p(\cdot, \cdot) = \langle \cdot, \cdot \rangle_p$ on each tangent space $T_p M$. This endows each tangent space with a Hilbert space structure.</li>
    </ul>
    <div class="theorem-box">
        <p><strong>Definition: The Riemannian Gradient.</strong> Given a smooth function $E: M \to \mathbb{R}$, its gradient $\grad_p E$ at a point $p$ is the unique vector in $T_p M$ that represents the derivative $dE_p$ via the metric:</p>
        \[ g_p(\grad_p E, v) = dE_p(v) \quad \text{for all } v \in T_p M \]
    </div>
    <p>The existence and uniqueness of $\grad_p E$ are guaranteed by the Riesz Representation Theorem on the Hilbert space $(T_p M, g_p)$.</p>

    <h3>Gradient Example 1: The Space of Matrices with Frobenius Metric</h3>
    <p>The manifold $(M_{d \times n}, g_F)$ with the standard Frobenius metric $g_W(Z_1, Z_2) = \text{Tr}(Z_1^T Z_2)$ is isomorphic to Euclidean space $(\mathbb{R}^{dn}, g_E)$. In this standard case, the Riemannian gradient is simply the standard element-wise gradient of the loss function, arranged as a matrix.</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \grad_W E = \nabla E(W) \]
    </div>
    
    <h3>Gradient Example 2: A Preconditioned Metric</h3>
    <p>Now, let's change the geometry. Let $A$ be a fixed, symmetric positive-definite (SPD) operator. Define a new inner product $g_W^A(Z_1, Z_2) = \text{Tr}(Z_1^T A^{-1} Z_2)$. To find the Riemannian gradient $\grad_W^A E$ under this new metric, we solve the duality condition:</p>
    \[ g_W^A(\grad_W^A E, Z) = dE_W(Z) \]
    <p>Substituting the definitions for the metric and the directional derivative (where $\nabla E(W)$ is the standard gradient):</p>
    \[ \text{Tr}((\grad_W^A E)^T A^{-1} Z) = \text{Tr}((\nabla E(W))^T Z) \]
    <p>Since this must hold for all tangent vectors $Z$, we can equate the matrix terms, which yields:</p>
    <div class="theorem-box" style="text-align:center;">
        \[ \grad_W^A E = A \nabla E(W) \]
    </div>
    <p><strong>The Takeaway:</strong> The gradient of a function is not absolute; it depends on the chosen geometry (metric). Changing the metric from the standard one to a preconditioned one transforms the gradient itself.</p>

    <h2>Matrix Decompositions</h2>
    <p>We will also need two fundamental tools from linear algebra: the Singular Value Decomposition (SVD) and the Polar Decomposition.</p>
    <h3>Singular Value Decomposition (SVD)</h3>
    <div class="theorem-box">
        <p><strong>Theorem.</strong> For any real matrix $M \in \mathbb{R}^{m \times n}$, there exists a decomposition $ M = U \Sigma V^T $, where:</p>
    </div>
    <ul>
        <li>$U \in \mathbb{R}^{m \times m}$ is an <strong>orthogonal</strong> matrix (left singular vectors).</li>
        <li>$V \in \mathbb{R}^{n \times n}$ is an <strong>orthogonal</strong> matrix (right singular vectors).</li>
        <li>$\Sigma \in \mathbb{R}^{m \times n}$ is a rectangular diagonal matrix of non-negative singular values.</li>
    </ul>

    <h3>Polar Decompositions</h3>
    <p>Any square matrix $W \in M_d$ can be decomposed into a "stretch" (a symmetric positive semi-definite matrix) and a "rotation" (an orthogonal matrix).</p>
    <ul>
        <li><strong>Left Polar Decomposition:</strong> $W = QP$, where $P = \sqrt{W^T W}$.</li>
        <li><strong>Right Polar Decomposition:</strong> $W = RU^T$, where $R = \sqrt{W W^T}$.</li>
    </ul>
    
    <h3>Connection between SVD & Polar Decompositions</h3>
    <p>The two decompositions are intrinsically linked. Given the SVD of $W = U \Sigma V^T$, we can construct the polar factors:</p>
    <ul>
        <li>Stretch matrices: $P = V \Sigma V^T$ and $R = U \Sigma U^T$.</li>
        <li>Orthogonal matrices: $Q = U V^T$.</li>
    </ul>
    <p>Conversely, given the polar form $W=QP$, we can find the SVD by taking the eigendecomposition of the stretch matrix $P = V \Sigma V^T$ and substituting it back in: $W = Q (V \Sigma V^T) = (Q V) \Sigma V^T$. This is the SVD of $W$.</p>

    <h1 style="border-bottom:none; margin-top: 3em;">Thank You!</h1>
    <h3 style="text-align:center; color:#444; margin-top:-1.2em;">Any Questions?</h3>

</div>
</body>
</html>