# -*- coding: utf-8 -*-
"""GPT_math_queue_GPT_html.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LmY_y-oiZZjSeX-Uu2ORVrIfzKiI4akj
"""

# @title Installation

!pip install "pydantic-ai-slim[openai,logfire,mcp]"

import os
import json
import re
import html as html_lib
from typing import Optional, Literal, Dict, List, Any
from pydantic import BaseModel, Field
import openai
from IPython.display import display, HTML
import requests
import random
from pydantic_ai import Agent, RunContext
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openrouter import OpenRouterProvider
from pydantic_ai.mcp import MCPServerStdio
from google.colab import userdata
import requests
from bs4 import BeautifulSoup
import logfire
import logging
from google.colab import drive
#from pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings

# @title  Logfire
logfire.configure(token='pylf_v1_us_fRywPwpDL81M7PXZHWMfFVSpTrKq2PNfbSbd86dZhblk')
logfire.instrument_pydantic_ai()

# @title Logging

logger = logging.getLogger()
logger.setLevel(logging.DEBUG)

# Mount Google Drive
try:
    drive.mount('/content/drive')
    # Define the base directory in Google Drive where you want to save the files
    # You might want to change 'problem_outputs' to a specific folder name
    google_drive_base_dir = '/content/drive/MyDrive/Colab Notebooks/problem_outputs'
    if not os.path.exists(google_drive_base_dir):
        os.makedirs(google_drive_base_dir)
        logging.info(f"Created Google Drive base directory: {google_drive_base_dir}")
except Exception as e:
    logging.error(f"Error mounting Google Drive: {e}")
    google_drive_base_dir = None # Set to None if mounting fails

def save_to_file(filename, content):
    """Saves content to a specified file in Google Drive and returns the full Drive path."""
    if google_drive_base_dir is None:
        logging.error("Google Drive not mounted or base directory not set. Cannot save file.")
        return filename  # Return just the filename if saving fails

    # Construct the full path in Google Drive
    drive_filepath = os.path.join(google_drive_base_dir, filename)

    try:
        # Ensure the directory structure exists within the Google Drive base directory
        os.makedirs(os.path.dirname(drive_filepath), exist_ok=True)

        with open(drive_filepath, 'w', encoding='utf-8') as f:
            if isinstance(content, (dict, list)):
                json.dump(content, f, indent=2, ensure_ascii=False)
            else:
                f.write(str(content))

        logging.info(f"Successfully saved file to Google Drive: {drive_filepath}")

        # Note: Creating a direct clickable link to a Google Drive file in Colab
        # is not as straightforward as local files. We return the full path.
        # You can navigate to this path in the Colab file browser or Google Drive.
        return drive_filepath
    except Exception as e:
        logging.error(f"Error saving file to Google Drive {drive_filepath}: {e}")
        return filename  # Return just the filename if saving fails


# -------------------------
# JSON -> aesthetic HTML
# -------------------------

DEFAULT_MATHJAX_MACROS: Dict[str, Any] = {
    "Tr": "\\\\operatorname{Tr}",
    "Herm": "\\\\mathrm{Herm}",
    "argmin": ["\\\\operatorname*{argmin}"],
    "diag": "\\\\operatorname{diag}",
    "ri": "\\\\mathrm{ri}",
}


def infer_mathjax_macros(text: str, extra_macros: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """Return the MathJax macros needed for `text`.

    We always include `DEFAULT_MATHJAX_MACROS` (matches your template), and
    also include any `extra_macros` you pass.

    If you later add more macros to DEFAULT_MATHJAX_MACROS, this will
    automatically pick them up.
    """
    macros: Dict[str, Any] = {}

    # Include defaults (template parity)
    for name, definition in DEFAULT_MATHJAX_MACROS.items():
        # Only include defaults that are actually used OR are part of the base template.
        # Here we keep base template macros always to match your reference HTML.
        macros[name] = definition

    if extra_macros:
        macros.update(extra_macros)

    # If you want "only used" instead of "always include", flip this logic.
    # Kept as-is to preserve exact template behavior.
    return macros


def _js_string_literal(s: str) -> str:
    # Minimal JS string escaping for our config literals.
    return "\"" + s.replace("\\\\", "\\\\\\\\").replace("\"", "\\\\\"") + "\""


def _macros_to_js_object(macro_dict: Dict[str, Any]) -> str:
    parts: List[str] = []
    for k, v in macro_dict.items():
        if isinstance(v, list):
            # Matches the style used in your reference HTML (e.g. argmin)
            inner = ", ".join(_js_string_literal(str(x)) for x in v)
            parts.append(f"{k}: [{inner}]")
        else:
            parts.append(f"{k}: {_js_string_literal(str(v))}")
    return "{\n              " + ",\n              ".join(parts) + "\n            }"


def build_html_doc(*, body_html: str, title: str, macros: Dict[str, Any]) -> str:
    macros_js = _macros_to_js_object(macros)

    # Template matches ai_studio_code*.html (MathJax config + CSS)
    return f"""<!DOCTYPE html>
<html>
<head>
<title>{html_lib.escape(title, quote=False)}</title>
<script>
    MathJax = {{
        tex: {{
            inlineMath: [['$', '$'], ['\\\\(', '\\\\)']],
            displayMath: [['$$', '$$'], ['\\\\[', '\\\\]']],
            macros: {macros_js},
            packages: {{'[+]': ['physics', 'ams']}}
        }},
        svg: {{
            fontCache: 'global'
        }}
    }};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
<style>
  body {{
    font-family: serif;
    line-height: 1.6;
    margin: 2em;
    max-width: 800px;
    margin-left: auto;
    margin-right: auto;
  }}
  h3, h4 {{
    font-family: sans-serif;
    color: #2c3e50;
    border-bottom: 1px solid #ccc;
    padding-bottom: 5px;
  }}
  ul {{
    list-style-type: disc;
    margin-left: 20px;
  }}
  ol {{
    list-style-type: decimal;
    margin-left: 20px;
  }}
  .interpretation {{
    background-color: #f8f9fa;
    border-left: 3px solid #777;
    padding: 10px 15px;
    margin: 1em 0;
  }}
  code {{
    background-color: #eee;
    padding: 2px 4px;
    border-radius: 3px;
  }}
  pre code {{
    display: block;
    padding: 12px;
    overflow-x: auto;
  }}
  blockquote {{
    border-left: 3px solid #bbb;
    margin: 1em 0;
    padding: 0.25em 0.75em;
    background: #fafafa;
  }}
</style>
</head>
<body>

<div>
{body_html}
</div>

</body>
</html>"""


_inline_bold_re = re.compile(r"\*\*(.+?)\*\*")
_inline_code_re = re.compile(r"`([^`]+?)`")
_ol_item_re = re.compile(r"^\s*(\d+)\.\s+(.*)$")
_ul_item_re = re.compile(r"^\s*([-*])\s+(.*)$")


def _format_inline(text_escaped: str) -> str:
    # `code` first, then **bold**
    text_escaped = _inline_code_re.sub(r"<code>\1</code>", text_escaped)
    text_escaped = _inline_bold_re.sub(r"<b>\1</b>", text_escaped)
    return text_escaped


def markdownish_to_html(text: str) -> str:
    """Convert the notebook's markdown-ish outputs to HTML.

    Guarantees:
    - Does not alter underlying words/math (only wraps structure).
    - Escapes HTML in the source so raw `<...>` won't break the document.
    """

    lines = text.splitlines()

    out: List[str] = []
    para: List[str] = []

    in_code_block = False
    code_lines: List[str] = []

    in_blockquote = False
    quote_lines: List[str] = []

    list_mode: Optional[str] = None  # 'ul'|'ol'

    def flush_paragraph() -> None:
        nonlocal para
        if not para:
            return
        joined = "\n".join(para)
        out.append(f"<p>{joined}</p>")
        para = []

    def close_list() -> None:
        nonlocal list_mode
        if list_mode is None:
            return
        out.append(f"</{list_mode}>")
        list_mode = None

    def close_blockquote() -> None:
        nonlocal in_blockquote, quote_lines
        if not in_blockquote:
            return
        # Each quote line is treated as a line in a paragraph
        joined = "<br/>\n".join(quote_lines)
        out.append(f"<blockquote><p>{joined}</p></blockquote>")
        in_blockquote = False
        quote_lines = []

    def close_code_block() -> None:
        nonlocal in_code_block, code_lines
        if not in_code_block:
            return
        code_text = "\n".join(code_lines)
        out.append(f"<pre><code>{code_text}</code></pre>")
        in_code_block = False
        code_lines = []

    for raw_line in lines:
        line = raw_line.rstrip("\n")

        # Fenced code blocks
        if line.strip().startswith("```"):
            close_blockquote()
            close_list()
            flush_paragraph()
            if in_code_block:
                close_code_block()
            else:
                in_code_block = True
                code_lines = []
            continue

        if in_code_block:
            code_lines.append(html_lib.escape(line, quote=False))
            continue

        # Blank line
        if line.strip() == "":
            close_blockquote()
            close_list()
            flush_paragraph()
            continue

        # Blockquote line
        if line.lstrip().startswith(">"):
            close_list()
            flush_paragraph()
            in_blockquote = True
            q = line.lstrip()[1:]
            if q.startswith(" "):
                q = q[1:]
            q_esc = _format_inline(html_lib.escape(q, quote=False))
            quote_lines.append(q_esc)
            continue
        else:
            close_blockquote()

        # Horizontal rule
        if line.strip() == "---":
            close_list()
            flush_paragraph()
            out.append("<hr/>")
            continue

        # Headings
        if line.startswith("#### "):
            close_list()
            flush_paragraph()
            out.append(f"<h4>{_format_inline(html_lib.escape(line[5:], quote=False))}</h4>")
            continue
        if line.startswith("### "):
            close_list()
            flush_paragraph()
            out.append(f"<h4>{_format_inline(html_lib.escape(line[4:], quote=False))}</h4>")
            continue
        if line.startswith("## "):
            close_list()
            flush_paragraph()
            out.append(f"<h3>{_format_inline(html_lib.escape(line[3:], quote=False))}</h3>")
            continue

        # Lists
        m_ol = _ol_item_re.match(line)
        m_ul = _ul_item_re.match(line)
        if m_ol:
            flush_paragraph()
            if list_mode != "ol":
                close_list()
                out.append("<ol>")
                list_mode = "ol"
            item = _format_inline(html_lib.escape(m_ol.group(2), quote=False))
            out.append(f"<li>{item}</li>")
            continue
        if m_ul:
            flush_paragraph()
            if list_mode != "ul":
                close_list()
                out.append("<ul>")
                list_mode = "ul"
            item = _format_inline(html_lib.escape(m_ul.group(2), quote=False))
            out.append(f"<li>{item}</li>")
            continue

        # Default: paragraph line (preserve line breaks)
        close_list()
        para.append(_format_inline(html_lib.escape(line, quote=False)))

    close_code_block()
    close_blockquote()
    close_list()
    flush_paragraph()

    return "\n".join(out)


def jsonish_to_body_html(content: Any) -> str:
    if isinstance(content, (dict, list)):
        dumped = json.dumps(content, indent=2, ensure_ascii=False)
        dumped_esc = html_lib.escape(dumped, quote=False)
        return f"<pre><code>{dumped_esc}</code></pre>"

    return markdownish_to_html(str(content))


def sibling_html_filename(json_filename: str) -> str:
    if json_filename.lower().endswith('.json'):
        return json_filename[:-5] + '.html'
    return json_filename + '.html'


def save_json_and_html(
    filename: str,
    content: Any,
    *,
    doc_title: Optional[str] = None,
    extra_macros: Optional[Dict[str, Any]] = None,
    display_html_preview: bool = False,
):
    """Save the original output and a sibling HTML rendering to Drive."""

    saved_json_path = save_to_file(filename, content)

    html_filename = sibling_html_filename(filename)

    # Infer macros based on the exact content
    if isinstance(content, (dict, list)):
        macro_text = json.dumps(content, ensure_ascii=False)
    else:
        macro_text = str(content)

    macros = infer_mathjax_macros(macro_text, extra_macros=extra_macros)

    body_html = jsonish_to_body_html(content)
    title = doc_title or os.path.splitext(os.path.basename(filename))[0]
    html_doc = build_html_doc(body_html=body_html, title=title, macros=macros)

    saved_html_path = save_to_file(html_filename, html_doc)

    if display_html_preview:
        display(HTML(html_doc))

    return saved_json_path, saved_html_path


# # Create a directory for this problem's outputs
# problem_statement = "Testing"
# problem_dir = problem_statement.strip().splitlines()[0].replace('#', '').strip().replace(' ', '_').replace('$', '')
# if not os.path.exists(problem_dir):
#     os.makedirs(problem_dir)
#     logging.info(f"Created directory for problem outputs: {problem_dir}")
# solution_filename = os.path.join(problem_dir, f"initial_solution_hint_{1}.json")
# solution = "Test2"
# solution_link = save_to_file(solution_filename, solution)
# logging.info(f"Initial Solution {1} ({solution_link}): {solution}")

# @title Configuration for the Solver-Verifier Pipeline based on arXiv:2507.15855v2

pipeline_config = {
    "solver": {
        "model": "openai/gpt-5.2",
        "temperature": 0.1,
        "max_tokens": 32768,
        "system_prompt": """
### Core Instructions ###

*   **Rigor is Paramount:** Your primary goal is to produce a complete and rigorously justified solution. Every step in your solution must be logically sound and clearly explained. A correct final answer derived from flawed or incomplete reasoning is considered a failure.
*   **Honesty About Completeness:** If you cannot find a complete solution, you must **not** guess or create a solution that appears correct but contains hidden flaws or justification gaps. Instead, you should present only significant partial results that you can rigorously prove. A partial result is considered significant if it represents a substantial advancement toward a full solution. Examples include:
    *   Proving a key lemma.
    *   Fully resolving one or more cases within a logically sound case-based proof.
    *   Establishing a critical property of the mathematical objects in the problem.
    *   For an optimization problem, proving an upper or lower bound without proving that this bound is achievable.
*   **Use TeX for All Mathematics:** All mathematical variables, expressions, and relations must be enclosed in TeX delimiters (e.g., ‘Let $n$ be an integer.’).

### Output Format ###

Your response MUST be structured into the following sections, in this exact order.

**1. Summary**

Provide a concise overview of your findings. This section must contain two parts:

*   **a. Verdict:** State clearly whether you have found a complete solution or a partial solution.
    *   **For a complete solution:** State the final answer, e.g., "I have successfully solved the problem. The final answer is..."
    *   **For a partial solution:** State the main rigorous conclusion(s) you were able to prove, e.g., "I have not found a complete solution, but I have rigorously proven that..."
*   **b. Method Sketch:** Present a high-level, conceptual outline of your solution. This sketch should allow an expert to understand the logical flow of your argument without reading the full detail. It should include:
    *   A narrative of your overall strategy.
    *   The full and precise mathematical statements of any key lemmas or major intermediate results.
    *   If applicable, describe any key constructions or case splits that form the backbone of your argument.

**2. Detailed Solution**

Present the full, step-by-step mathematical proof. Each step must be logically justified and clearly explained. The level of detail should be sufficient for an expert to verify the correctness of your reasoning without needing to fill in any gaps. This section must contain ONLY the complete, rigorous proof, free of any internal commentary, alternative approaches, or failed attempts.

### Self-Correction Instruction ###

Before finalizing your output, carefully review your "Method Sketch" and "Detailed Solution" to ensure they are clean, rigorous, and strictly adhere to all instructions provided above. Verify that every statement contributes directly to the final, coherent mathematical argument.
"""
    },
    "verifier": {
        "model": "openai/gpt-5.2",
        "temperature": 0.1, # A low temperature is also good for verification tasks
        "system_prompt": """
You are an expert mathematician and a meticulous grader for an International Mathematical Olympiad (IMO) level exam. Your primary task is to rigorously verify the provided mathematical solution. A solution is to be judged correct **only if every step is rigorously justified.** A solution that arrives at a correct final answer through flawed reasoning, educated guesses, or with gaps in its arguments must be flagged as incorrect or incomplete.

### Instructions ###

**1. Core Instructions**
*   Your sole task is to find and report all issues in the provided solution. You must act as a **verifier**, NOT a solver. **Do NOT attempt to correct the errors or fill the gaps you find.**
*   You must perform a **step-by-step** check of the entire solution. This analysis will be presented in a **Detailed Verification Log**, where you justify your assessment of each step: for correct steps, a brief justification suffices; for steps with errors or gaps, you must provide a detailed explanation.

**2. How to Handle Issues in the Solution**
When you identify an issue in a step, you MUST first classify it into one of the following two categories and then follow the specified procedure.

*   **a. Critical Error:**
    This is any error that breaks the logical chain of the proof. This includes both **logical fallacies** (e.g., claiming that ‘A>B, C>D` implies ‘A-C>B-D‘) and **factual errors** (e.g., a calculation error like '2+3=6').
    *   **Procedure:**
        *   Explain the specific error and state that it **invalidates the current line of reasoning**.
        *   Do NOT check any further steps that rely on this error.
        *   You MUST, however, scan the rest of the solution to identify and verify any fully independent parts. For example, if a proof is split into multiple cases, an error in one case does not prevent you from checking the other cases.

*   **b. Justification Gap:**
    This is for steps where the conclusion may be correct, but the provided argument is incomplete, hand-wavy, or lacks sufficient rigor.
    *   **Procedure:**
        *   Explain the gap in the justification.
        *   State that you will **assume the step's conclusion is true** for the sake of argument.
        *   Then, proceed to verify all subsequent steps to check if the remainder of the argument is sound.

**3. Output Format**
Your response MUST be structured into two main sections: a **Summary** followed by the **Detailed Verification Log**.

*   **a. Summary**
    This section MUST be at the very beginning of your response. It must contain two components:
    *   **Final Verdict**: A single, clear sentence declaring the overall validity of the solution. For example: "The solution is correct," "The solution contains a Critical Error and is therefore invalid," or "The solution's approach is viable but contains several Justification Gaps."
    *   **List of Findings**: A bulleted list that summarizes **every** issue you discovered. For each finding, you must provide:
        *   **Location:** A direct quote of the key phrase or equation where the issue occurs.
        *   **Issue:** A brief description of the problem and its classification (**Critical Error** or **Justification Gap**).

*   **b. Detailed Verification Log**
    Following the summary, provide the full, step-by-step verification log as defined in the Core Instructions. When you refer to a specific part of the solution, **quote the relevant text** to make your reference clear before providing your detailed analysis of that part.
"""
    }
}

def call_openrouter_api(user_prompt: str, context: str, api_key: str, model: str = "google/gemini-2.5-pro", temperature: float = 0.1, reasoning: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Calls the OpenRouter API with a custom user prompt and context, including an optional reasoning parameter.

    Args:
        user_prompt: The user's prompt for the model.
        context: The context to provide to the model.
        api_key: The OpenRouter API key.
        model: The name of the model to use (defaults to "google/gemini-2.5-pro").
        temperature: The temperature for the API call (defaults to 0.1).
        max_tokens: The maximum number of tokens for the API call (defaults to 4096).
        reasoning: An optional dictionary for the reasoning parameter.

    Returns:
        A dictionary containing the JSON response from the API, or an empty dictionary
        if an error occurred or the response could not be parsed.
    """
    prompt_messages = [
        {"role": "user", "content": f"{user_prompt}\n\nContext:\n---\n{context}\n---"}
    ]

    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "openai/gpt-5.2",
        "messages": prompt_messages,
        "temperature": temperature,
        "reasoning": {"effort": "high"},
        "provider": {
            "order": ["google-ai-studio"]
        }
    }
    # if reasoning:
    #     payload["reasoning"] = reasoning


    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload))
        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)
        response_data = response.json()

        # Attempt to parse the JSON response content from the model
        try:
            # Assuming the model's response is a JSON string, potentially wrapped in markdown
            model_response_content = response_data['choices'][0]['message']['content']

            # Check if the response content is wrapped in a markdown code block
            if model_response_content.strip().startswith("```json") and model_response_content.strip().endswith("```"):
                # Extract the JSON string from within the markdown code block
                json_string = model_response_content.strip()[len("```json"): -len("```")].strip()
            else:
                json_string = model_response_content.strip()


            analysis_result = json.loads(json_string)
            return analysis_result
        except (json.JSONDecodeError, KeyError) as e:
            print(f"Error parsing model response as JSON: {e}")
            return response_data.get('choices', [{}])[0].get('message', {}).get('content', 'N/A')

    except requests.exceptions.RequestException as e:
        print(f"Error calling OpenRouter API using requests: {e}")
        return {} # Return empty dictionary on API error

# @title hint generator

def generate_hints(problem_statement, num_hints, likelihood):
    """
    Generates hints for a given problem statement using the OpenRouter API.

    Args:
      problem_statement: The problem statement for which hints are to be generated.
      num_hints: The number of hints to generate.

    Returns:
      A list of strictly different approaches to get started on the problem,
      or None if an error occurred.
    """
    api_key = userdata.get('OPENROUTER_API_KEY')
    if not api_key:
        print("OpenRouter API key not found.")
        return None

    user_prompt = f"Think step by step and generate exactly {num_hints} distinct starting hints or principled approaches (possibly from different areas of math) from the entire distribution of hints for the following mathematical problem. Your response should be a JSON array of strings, without any other comments. In each string, every single ‘\’ must be written as ‘\\’ so that the result is valid JSON."

    response = call_openrouter_api(
        user_prompt=user_prompt,
        context=problem_statement,
        api_key=api_key,
        model="openai/gpt-5.2", # Or another suitable model
        temperature=0.7, # Higher temperature for creativity
    )

    # Assuming the response is a dictionary with a key containing the list of hints
    # Adjust this based on the actual API response structure if needed
    if response and isinstance(response, list):
        return response
    else:
        print("Failed to generate hints or parse API response.")
        return None

# @title initial solution generator

def generate_initial_solution(problem_statement, hint):
    """
    Generates an initial solution based on the problem statement and a hint
     using the OpenRouter API and the solver's system prompt.

    Args:
      problem_statement: The problem statement.
      hint: A specific hint to guide the solution generation.

    Returns:
      The response from the API call, or None if an error occurred.
    """
    api_key = userdata.get('OPENROUTER_API_KEY')
    if not api_key:
        print("OpenRouter API key not found.")
        return None

    user_prompt = f"""
Solve the following mathematical problem, guided by the provided hint.

Problem Statement:
---
{problem_statement}
---

Hint:
---
{hint}
---

Provide a complete and rigorous solution following the specified output format in the system prompt.
"""
    solver_config = pipeline_config['solver']

    response = call_openrouter_api(
        user_prompt=user_prompt,
        context=solver_config['system_prompt'], # Use the system prompt as context
        api_key=api_key,
        model="openai/gpt-5.2",
        temperature=solver_config['temperature'],
    )

    return response

# @title solution improver

def self_improve(problem_statement, solution, hint):
    """
    Instructs the model to improve a given solution using the OpenRouter API
    and the solver's system prompt.

    Args:
      problem_statement: The original problem statement.
      solution: The existing solution to be improved.

    Returns:
      The improved solution from the API call, or None if an error occurred.
    """
    api_key = userdata.get('OPENROUTER_API_KEY')
    if not api_key:
        print("OpenRouter API key not found.")
        return None

    user_prompt = f"""
Review the provided solution for the following mathematical problem and improve it based on the core instructions and output format in the system prompt. Ensure the improved solution is more rigorous, complete, and adheres strictly to the required structure.

Problem Statement:
---
{problem_statement}
---

Hint:
---
{hint}
---

Existing Solution:
---
{solution}
---

Provide the improved solution following the specified output format in the system prompt.
"""
    solver_config = pipeline_config['solver']

    response = call_openrouter_api(
        user_prompt=user_prompt,
        context=solver_config['system_prompt'], # Use the solver system prompt as context
        api_key=api_key,
        model="openai/gpt-5.2",
        temperature=solver_config['temperature'],
    )

    return response

# @title verifier

def verify_solution(problem_statement, solution, hint):
    """
    Verifies a given solution using the OpenRouter API and the verifier's
    system prompt.

    Args:
      problem_statement: The original problem statement.
      solution: The solution to be verified.

    Returns:
      A dictionary containing the verification results, or False if the solution
      is deemed correct, or None if an error occurred.
    """
    api_key = userdata.get('OPENROUTER_API_KEY')
    if not api_key:
        print("OpenRouter API key not found.")
        return None

    user_prompt = f"""
Rigorously verify the following mathematical solution against the problem statement and hint according to the instructions and output format in the system prompt.

Problem Statement:
---
{problem_statement}
---

Hint:
---
{hint}
---

Solution to Verify:
---
{solution}
---

Provide your verification results following the specified output format in the system prompt, including a Summary and a Detailed Verification Log. The Summary must contain a 'Final Verdict' field.
"""
    verifier_config = pipeline_config['verifier']

    response = call_openrouter_api(
        user_prompt=user_prompt,
        context=verifier_config['system_prompt'], # Use the verifier system prompt as context
        api_key=api_key,
        model="openai/gpt-5.2",
        temperature=verifier_config['temperature'],
    )

    # Check if the string "the solution is correct" is present in the string representation of the response (case-insensitive)
    if isinstance(response, dict):
         response_str = json.dumps(response).lower()
         if "the solution is correct" in response_str:
            return False # Indicate successful verification

    # If the string is not found or the response is not a dictionary, return the response
    return response

# @title bug report reviewer

def review_bug_report(problem_statement, solution, hint, bug_report: Optional[Dict[str, Any]]) -> tuple[Optional[Dict[str, Any]], int]:
    """
    Reviews a bug report from the verifier using an API call to determine if it
    indicates a major issue and provides a critique of the bug report itself.

    Args:
      bug_report: The dictionary output from the verify_solution function,
                  or None if verification failed.

    Returns:
      A tuple containing:
        - The reviewed bug report from the API (or None if verification failed or API call failed).
        - An integer indicating the number of major issues found.
    """
    if bug_report is None:
        # If there is no bug report, treat it as a major issue as verification failed
        print("Verification failed, treating as a major issue.")
        return None, 1 # Assuming a failed verification is one major issue

    api_key = userdata.get('OPENROUTER_API_KEY')
    if not api_key:
        print("OpenRouter API key not found.")
        return None, 1 # Treat missing API key as a major issue

    user_prompt = f"""
Review the following bug report generated by a mathematical solution verifier. Analyze the reported issues and provide a critique. Specifically:
1. The Locations and Issues pointed out by the bug report.
2. Are the issues identified by the verifier genuinely major issues (Critical Errors) or minor issues (Justification Gaps)?
3. Are there any inconsistencies or errors in the bug report itself?
4. Based on the bug report, how many major issues (Critical Errors) are indicated in the original solution?

Provide your analysis and the count of major issues as a JSON object with two keys:
- "critique": A string containing your analysis and critique of the bug report.
- "major_issue_count": An integer representing the number of major issues (Critical Errors) indicated in the bug report.

Bug Report to Review:
---
{json.dumps(bug_report, indent=2)}
---
"""
    # Using a general model for reviewing the bug report
    response = call_openrouter_api(
        user_prompt=user_prompt,
        context="", # No specific system prompt needed for this task
        api_key=api_key,
        model="openai/gpt-5,2", # Or another suitable model for analysis
        temperature=0.1,
    )

    if response and isinstance(response, dict):
        critique = response.get('critique')
        major_issue_count = response.get('major_issue_count', 0)
        if critique is None or not isinstance(major_issue_count, int):
             print("API response for bug report review is not in the expected format.")
             return response, 1 # Treat unexpected format as a major issue
        return response, major_issue_count
    else:
        print("Failed to review bug report or parse API response.")
        return None, 1 # Treat failed API call as a major issue

# @title solution correction

def correct_solution(problem_statement, solution, hint, reviewed_bug_report):
    """
    Corrects a given solution based on a reviewed bug report using the
    OpenRouter API and the solver's system prompt.

    Args:
      problem_statement: The original problem statement.
      solution: The previous solution that needs correction.
      reviewed_bug_report: The reviewed bug report dictionary from
                           review_bug_report.

    Returns:
      The corrected solution from the API call, or None if an error occurred.
    """
    api_key = userdata.get('OPENROUTER_API_KEY')
    if not api_key:
        print("OpenRouter API key not found.")
        return None

    user_prompt = f"""
Correct the following mathematical solution based on the provided bug report and the original problem statement and hint. Ensure the corrected solution is rigorous, complete, and strictly adheres to the output format specified in the system prompt. Address all issues raised in the bug report.

Problem Statement:
---
{problem_statement}
---

Hint:
---
{hint}
---

Previous Solution:
---
{solution}
---

Bug Report:
---
{json.dumps(reviewed_bug_report, indent=2)}
---

Provide the corrected solution following the specified output format in the system prompt.
"""
    solver_config = pipeline_config['solver']

    response = call_openrouter_api(
        user_prompt=user_prompt,
        context=solver_config['system_prompt'], # Use the solver system prompt as context
        api_key=api_key,
        model=solver_config['model'],
        temperature=solver_config['temperature'],
    )

    return response

def solve_problem(problem_statement, num_initial_samples, likelihood, user_hints=None):
  from collections import deque

  hints = generate_hints(problem_statement, num_initial_samples, likelihood)
  if user_hints and hints:
    user_hints.extend(hints)
    hints = user_hints
  elif user_hints:
    hints = user_hints
  elif hints is None:
    logging.warning("Failed to generate or receive any hints. Skipping problem solving.")
    return []

  logging.info(f"Generated hints: {hints}")

  problem_dir = problem_statement.strip().splitlines()[0].replace("#", "").strip().replace(" ", "_").replace("$", "")
  if not os.path.exists(problem_dir):
    os.makedirs(problem_dir)
    logging.info(f"Created directory for problem outputs: {problem_dir}")

  queue_structure = {}
  queue_names = []
  final_solutions = []

  def enqueue_job(job_type, jobs_deque, payload=None):
    job_payload = payload if payload is not None else {}
    jobs_deque.append({"type": job_type, "payload": job_payload})

  for index, hint in enumerate(hints, start=1):
    queue_name = f"hint_{index}"
    queue_structure[queue_name] = {
      "jobs": deque([{"type": "generate_initial_solution"}]),
      "context": {
        "hint_index": index,
        "hint": hint,
        "current_solution": None,
        "initial_solution": None,
        "bug_report": None,
        "reviewed_bug_report": None,
        "major_issue_counter": 0,
        "consecutive_passes": 0,
        "iteration_count": 0
      }
    }
    queue_names.append(queue_name)

  def process_job(job, queue_context, jobs_deque):
    nonlocal final_solutions

    job_type = job["type"]
    idx = queue_context["hint_index"]

    if job_type == "generate_initial_solution":
      logging.info(f"Generating initial solution for hint {idx}")
      solution = generate_initial_solution(problem_statement, queue_context["hint"])
      queue_context["initial_solution"] = solution
      queue_context["current_solution"] = solution
      solution_filename = os.path.join(problem_dir, f"initial_solution_hint_{idx}.json")
      solution_link, solution_html_link = save_json_and_html(solution_filename, solution)
      logging.info(f"Initial Solution {idx} ({solution_link})")
      logging.info(f"Initial Solution {idx} HTML ({solution_html_link})")
      enqueue_job("improve_solution", jobs_deque)
      return

    if job_type == "improve_solution":
      logging.info(f"Improving initial solution {idx}")
      improved_solution = self_improve(problem_statement, queue_context["current_solution"], queue_context["hint"])
      queue_context["current_solution"] = improved_solution
      queue_context["consecutive_passes"] = 0
      queue_context["major_issue_counter"] = 0
      queue_context["iteration_count"] = 0
      improved_solution_filename = os.path.join(problem_dir, f"improved_solution_{idx}_iter_0.json")
      improved_solution_link, improved_solution_html_link = save_json_and_html(improved_solution_filename, improved_solution)
      logging.info(f"Improved Solution {idx} Iter 0 ({improved_solution_link})")
      logging.info(f"Improved Solution {idx} Iter 0 HTML ({improved_solution_html_link})")
      enqueue_job("verify_solution", jobs_deque)
      return

    if job_type == "verify_solution":
      queue_context["iteration_count"] += 1
      iteration = queue_context["iteration_count"]
      logging.info(f"Verification iteration {iteration} for solution {idx}")
      current_solution = queue_context["current_solution"]
      bug_report = verify_solution(problem_statement, current_solution, queue_context["hint"])
      bug_report_filename = os.path.join(problem_dir, f"bug_report_sol_{idx}_iter_{iteration}.json")

      if not bug_report:
        logging.info(f"Verification successful for solution {idx}, iteration {iteration}.")
        queue_context["consecutive_passes"] += 1
        passes = queue_context["consecutive_passes"]
        logging.info(f"Consecutive passes: {passes}")

        if queue_context["consecutive_passes"] >= 5:
          logging.info(f"FOUND A SOLUTION for initial solution {idx} after {iteration} iterations!")
          final_solutions.append(current_solution)
          final_solution_filename = os.path.join(problem_dir, f"final_solution_sol_{idx}.json")
          final_solution_link, final_solution_html_link = save_json_and_html(final_solution_filename, current_solution)
          logging.info(f"Final Solution saved to: {final_solution_link}")
          logging.info(f"Final Solution HTML saved to: {final_solution_html_link}")
        else:
          enqueue_job("verify_solution", jobs_deque)
        return

      bug_report_link, bug_report_html_link = save_json_and_html(bug_report_filename, bug_report)
      logging.info(f"Bug report generated ({bug_report_link}) for solution {idx}, iteration {iteration}.")
      logging.info(f"Bug report HTML generated ({bug_report_html_link}) for solution {idx}, iteration {iteration}.")
      queue_context["consecutive_passes"] = 0

      reviewed_bug_report, major_issue = review_bug_report(problem_statement, current_solution, queue_context["hint"], bug_report)
      reviewed_bug_report_filename = os.path.join(problem_dir, f"reviewed_bug_report_sol_{idx}_iter_{iteration}.json")
      reviewed_bug_report_link, reviewed_bug_report_html_link = save_json_and_html(reviewed_bug_report_filename, reviewed_bug_report)
      logging.info(f"Reviewed Bug Report ({reviewed_bug_report_link}) for solution {idx}, iteration {iteration}")
      logging.info(f"Reviewed Bug Report HTML ({reviewed_bug_report_html_link}) for solution {idx}, iteration {iteration}")
      logging.info(f"Major Issue Count: {major_issue}")

      if major_issue >= 1:
        queue_context["major_issue_counter"] += 1
      else:
        queue_context["major_issue_counter"] = 0

      if queue_context["major_issue_counter"] >= 10:
        logging.warning(f"Aborting corrections for solution {idx} due to repeated major issues.")
        passes = queue_context["consecutive_passes"]
        logging.info(f"Consecutive passes: {passes}")
        return

      current_solution = correct_solution(problem_statement, current_solution, queue_context["hint"], reviewed_bug_report)
      queue_context["current_solution"] = current_solution
      corrected_solution_filename = os.path.join(problem_dir, f"corrected_solution_sol_{idx}_iter_{iteration}.json")
      corrected_solution_link, corrected_solution_html_link = save_json_and_html(corrected_solution_filename, current_solution)
      logging.info(f"Corrected Solution ({corrected_solution_link}) for solution {idx}, iteration {iteration}")
      logging.info(f"Corrected Solution HTML ({corrected_solution_html_link}) for solution {idx}, iteration {iteration}")
      passes = queue_context["consecutive_passes"]
      logging.info(f"Consecutive passes: {passes}")
      enqueue_job("verify_solution", jobs_deque)
      return

    logging.warning(f"Unknown job type: {job_type}")

  logging.info("Worker starting synchronous breadth-first processing...")

  while any(queue_data["jobs"] for queue_data in queue_structure.values()):
    for name in queue_names:
      queue_data = queue_structure[name]
      jobs_deque = queue_data["jobs"]
      if not jobs_deque:
        continue

      queue_context = queue_data["context"]
      job = jobs_deque.popleft()
      job_type = job["type"]
      hint_idx = queue_context["hint_index"]
      logging.info(f"Worker picked job {job_type} for hint {hint_idx}")
      process_job(job, queue_context, jobs_deque)

  logging.info("All queues are empty. Worker finished.")

  return final_solutions

problem_statement = r"""
Approximate theory

**TASK**
With appropriate modifications wherever necessary, rigorously complete the program described in the html note below. Your output should use Tex and markdown, not html.

<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Complexity and Smoothness in Masked Diffusion</title>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                macros: {
                  E: '\\mathbb{E}',
                  X: '\\mathcal{X}',
                  A: '\\mathcal{A}',
                  L: '\\mathcal{L}',
                  htop: 'h_{\\text{top}}',
                  dsym: 'd_{\\text{sym}}',
                  KL: '\\text{D}_{\\text{KL}}',
                  CE: '\\text{CE}',
                  rho: '\\rho'
                }
            },
            svg: { fontCache: 'global' }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <style>
        body { font-family: -apple-system, sans-serif; background-color: #f0f0f0; color: #333; line-height: 1.6; margin: 0; }
        .presentation-container { position: relative; width: 100vw; height: 100vh; overflow: hidden; background-color: #f8f8f8; }
        .slide { background-color: #ffffff; border-radius: 8px; padding: 40px 60px; width: 100%; height: 100%; box-sizing: border-box; position: absolute; top: 0; left: 0; opacity: 0; visibility: hidden; transition: opacity 0.5s ease-in-out; z-index: 1; overflow-y: auto; display: flex; flex-direction: column; align-items: center; }
        .slide.active { opacity: 1; visibility: visible; z-index: 2; }
        h1, h2 { color: #0056b3; text-align: center; }
        h1 { font-size: 2.2em; margin-bottom: 20px; }
        h2 { font-size: 1.8em; border-bottom: 2px solid #eee; padding-bottom: 10px; width: 100%; }
        p, ul, ol { max-width: 900px; width: 100%; font-size: 1.1em; }
        .theorem-box { border: 2px solid #0056b3; border-radius: 8px; padding: 20px; margin: 20px 0; background-color: #f7faff; width: 90%; max-width: 850px; }
        .nav-button { position: fixed; bottom: 25px; background-color: #0056b3; color: white; border: none; padding: 10px 20px; border-radius: 20px; cursor: pointer; z-index: 10; }
        #prevBtn { left: 25px; } #nextBtn { right: 25px; }
        .slide-counter { position: fixed; bottom: 25px; left: 50%; transform: translateX(-50%); font-size: 0.9em; color: #666; }
    </style>
</head>
<body>
    <div class="presentation-container">
        <!-- Slide 1: Title -->
        <section class="slide active">
            <h1 style="margin-top:15vh;">Dimensionality and Regularity in Masked Diffusion</h1>
            <p style="text-align:center;">A Mathematically Rigorous Framework for Discrete Generative Modeling</p>
            <p style="text-align:center; color:#666; margin-top:50px;">This note formalizes how topological entropy and symbolic Lipschitz regularity control the error rates of masked diffusion models, analogous to manifold dimension and score smoothness in continuous spaces.</p>
        </section>

        <!-- Slide 2: Masked Diffusion Definitions -->
        <section class="slide">
            <h2>1. Masked Diffusion Models</h2>
            <p>Let $\A = \{1, \dots, M\}$ be a finite vocabulary. Masked (absorbing) diffusion defines a corruption process on sequences $x \in \A^n$ by replacing tokens with a special mask symbol $\dagger$.</p>
            <div class="theorem-box">
                <strong>Forward Process:</strong> A token $x_i$ at time $t$ is either its ground truth value (with probability $\alpha_t$) or the mask $\dagger$ (with probability $1-\alpha_t$).
            </div>
            <p>The generative model (reverse process) learns to predict the missing tokens $u$ given a partially masked context $c$. The objective minimizes a weighted integral of KL divergences:</p>
            $$ \mathcal{J}(\theta) \propto \int_0^T w(t) \E_{c} [ \KL( p^*( \cdot | c) || q_\theta(\cdot | c) ) ] dt $$
            <p>where $p^*$ is the true data distribution and $q_\theta$ is the model.</p>
        </section>

        <!-- Slide 3: Dimensionality in Discrete Space -->
        <section class="slide">
            <h2>2. The Dimensionality of Language</h2>
            <p>In continuous space, we assume data lies on a $d$-dimensional manifold. In discrete "symbolic" space, dimensionality is characterized by the growth rate of valid configurations, known as <strong>Topological Entropy</strong>.</p>
            <div class="theorem-box">
                <strong>Definition (Symbolic Dimension):</strong> Let $L_n(X)$ be the set of valid length-$n$ strings. The topological entropy is $\htop(X) := \lim_{n \to \infty} \frac{1}{n} \log |L_n(X)|$. We define the <strong>symbolic dimension</strong> as:
                $$ \dsym := \frac{\htop(X)}{\log M} \in [0, 1] $$
            </div>
            <p>Under the ultrametric $\rho(x, x') = M^{-k}$ (where $k$ is the first mismatch), the number of balls of radius $\epsilon$ needed to cover the data space grows as $N(\epsilon) \approx \epsilon^{-\dsym}$. This $\dsym$ is the direct analogue of manifold dimension $d$.</p>
        </section>

        <!-- Slide 4: Smoothness in Discrete Space -->
        <section class="slide">
            <h2>3. Symbolic Smoothness (Regularity)</h2>
            <p>Smoothness in continuous diffusion refers to the derivatives of the score. In discrete spaces, we use <strong>Lipschitz Regularity</strong> on the shift space.</p>
            <div class="theorem-box">
                <strong>Definition (Symbolic Smoothness):</strong> A loss function $\ell_\theta(c, u)$ is $L$-smooth if:
                $$ |\ell_\theta(c, u) - \ell_\theta(c', u)| \leq L \rho(c, c') $$
            </div>
            <p>This implies that the model's prediction depends primarily on "local" context (near the mask). Using a product orthonormal basis $\chi_\alpha$, this regularity is equivalent to a spectral decay condition:</p>
            $$ \sum_{|\alpha| > k} |\hat{f}(\alpha)| w(\alpha) \leq C \theta^k $$
            <p>where high-frequency components (long-range dependencies) decay exponentially.</p>
        </section>

        <!-- Slide 5: Error Decomposition -->
        <section class="slide">
            <h2>4. The Error Split</h2>
            <p>When using parallel unmasking (predicting multiple tokens at once), the total error $\mathcal{E}$ splits into two distinct components:</p>
            <div class="theorem-box">
                $$ \mathcal{E} = \underbrace{\sum_{i \in B} \KL(p^*_i || q_{\theta,i})}_{\text{Learning / CE Error}} + \underbrace{\KL(p^*_B || \prod p^*_i)}_{\text{Independence Error}} $$
            </div>
            <ol>
                <li><strong>Learning Error:</strong> How well the model fits the marginals. This depends on $n$, $\dsym$, and smoothness.</li>
                <li><strong>Independence Error:</strong> The cost of assuming tokens are independent during a single step. This is an algorithmic bias inherent to factorized sampling.</li>
            </ol>
        </section>

        <!-- Slide 6: Bounding the Learning Error -->
        <section class="slide">
            <h2>5. Complexity and Generalization</h2>
            <p>We bound the Learning Error using the <strong>Rademacher Complexity</strong> of the function class. The symbolic dimension $\dsym$ determines the "size" of the space the model must learn.</p>
            <div class="theorem-box">
                <strong>Theorem (Generalization Bound):</strong> Under $L$-smoothness and $\dsym$ dimensionality, the estimation error scales as:
                $$ R(\hat{\theta}) - R^* \approx O\left( \frac{L^{\dsym/2}}{\sqrt{n}} \right) $$
            </div>
            <p>This result is critical: it shows that if the data has low symbolic dimension ($\dsym \ll 1$), the model can generalize from far fewer samples $n$. The smoothness $L$ acts as a multiplier, determining how "sensitive" the error is to the underlying complexity.</p>
        </section>

        <!-- Slide 7: Summary of the Program -->
        <section class="slide">
            <h2>6. Summary of the Program</h2>
            <p>To provide rigorous guarantees for discrete diffusion, we follow this chain of reasoning:</p>
            <ul>
                <li><strong>Step 1:</strong> Model the data as a subshift with entropy $\htop$, defining the intrinsic dimension $\dsym$.</li>
                <li><strong>Step 2:</strong> Impose symbolic Lipschitz regularity on the hypothesis class (or prove it via spectral decay).</li>
                <li><strong>Step 3:</strong> Use the metric entropy $N(\epsilon) \sim \epsilon^{-\dsym}$ to bound the Rademacher complexity via Dudley's chaining integral.</li>
                <li><strong>Step 4:</strong> Separate the statistical estimation error (controlled by $n, \dsym, L$) from the algorithmic independence error (controlled by the sampling schedule).</li>
            </ul>
            <div class="theorem-box" style="text-align:center;">
                <strong>Conclusion:</strong> Low-dimensional discrete data ($\dsym$) is learnable with rate $n^{-1/2}$ provided the model is symbolically smooth.
            </div>
        </section>
    </div>

    <button id="prevBtn" class="nav-button">Previous</button>
    <button id="nextBtn" class="nav-button">Next</button>
    <div class="slide-counter">1 / 7</div>

    <script>
        const slides = document.querySelectorAll('.slide');
        const counter = document.querySelector('.slide-counter');
        let current = 0;
        function update() {
            slides.forEach((s, i) => s.classList.toggle('active', i === current));
            counter.innerText = `${current + 1} / ${slides.length}`;
            if (window.MathJax) MathJax.typeset();
        }
        document.getElementById('nextBtn').onclick = () => { if(current < slides.length-1) { current++; update(); } };
        document.getElementById('prevBtn').onclick = () => { if(current > 0) { current--; update(); } };
    </script>
</body>
</html>
"""

user_hints = []

solve_problem(problem_statement, 5, 0.1, user_hints)

# Some requests turning out blank. Do print analysis
# implement verbalized sampling cot and multi-turn
# is breadth first queueing happening after the first solution's bug report --> reviewed bug report --> corrected solution : YES