<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Linear Networks: Effect of Noise â€” PDF Style</title>
    <!-- MathJax 3 for rendering LaTeX math -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                macros: {
                  Tr: '\\operatorname{Tr}',
                  grad: '\\operatorname{grad}',
                  Pr: '\\operatorname{Pr}',
                  erank: '\\operatorname{erank}'
                },
                packages: {'[+]': ['physics', 'ams']}
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            margin: 0 auto;
            padding: 0 0 40px 0;
            background-color: #f0f0f0;
            color: #333;
            line-height: 1.7;
            max-width: 900px;
        }
        .pdf-container {
            background: #fff;
            margin: 40px auto 0 auto;
            padding: 40px 60px;
            border-radius: 10px;
            box-shadow: 0 4px 24px rgba(0,0,0,0.10);
        }
        h1, h2, h3, h4 {
            color: #0056b3;
            margin-top: 2.2em;
            margin-bottom: 0.7em;
            font-weight: 600;
        }
        h1 { font-size: 2.3em; margin-top: 0.5em; text-align: center; }
        h2 { font-size: 1.6em; border-bottom: 2px solid #a0aec0; padding-bottom: 0.2em; }
        h3 { font-size: 1.2em; color: #444; }
        h4 { font-size: 1.1em; color: #0056b3; }
        p, ul, ol {
            font-size: 1.13em;
            margin-bottom: 1.2em;
        }
        ul, ol { padding-left: 2em; }
        ul li { list-style-type: disc; }
        .theorem-box {
            border: 2px solid #0056b3;
            border-radius: 8px;
            padding: 20px 30px;
            margin: 30px 0;
            background-color: #f7faff;
            box-shadow: 0 2px 8px rgba(0, 86, 179, 0.08);
            text-align: center;
        }
        .theorem-box p, .theorem-box ol li { text-align: left; font-size: 1.1em; }
        .columns {
            display: flex;
            gap: 40px;
            flex-wrap: wrap;
        }
        .column-text, .column-details {
            flex: 1 1 350px;
            min-width: 280px;
        }
        .column-details {
            background: #f9faff;
            border-left: 3px solid #0056b3;
            border-radius: 4px;
            padding: 18px 22px;
            padding-top: 0.5em;
        }
        @media (max-width: 900px) {
            .pdf-container { padding: 18px 8px; }
            .columns { flex-direction: column; gap: 0; }
        }
    </style>
</head>
<body>
<div class="pdf-container">
    <h1>Deep Linear Networks</h1>
    <h3 style="text-align:center; color:#444; margin-top:-1.2em;">Session 4: Effect of Noise and Discretization</h3>
    <h3 style="text-align:center; color:#555; margin-top:0.2em; font-weight:500;">Rathindra Nath Karmakar</h3>

    <h2>References</h2>
    <ul style="margin-top: 1.5em; font-size: 1.05em;">
        <li style="margin-bottom: 12px;"><b>Motion by mean curvature and Dyson Brownian motion</b>, C.-P. Huang, D. Inauen, and G. Menon (2023)</li>
        <li style="margin-bottom: 12px;"><b>An entropy formula for the deep linear network</b>, G. Menon and T. Yu (2024)</li>
        <li style="margin-bottom: 12px;"><b>The geometry of the deep linear network</b>, G. Menon (2024)</li>
    </ul>

    <h2>Recap: Key Questions for Gradient Flow</h2>
    <p>For the gradient flow dynamics on a loss surface $\mathcal{L}(\mathbf{W})$:</p>
    <div class="theorem-box" style="margin-bottom:30px;">
        \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
    </div>
    <p>We want to understand:</p>
    <ul>
        <li>Convergence guarantees? (<em>Yes, for balanced cases</em>)</li>
        <li>Convergence rate? (<em>Can be accelerated by depth</em>)</li>
        <li>Characterization of the minimizer? (<em>Bias towards max volume</em>)</li>
        <li><strong>Effect of noise and discretization?</strong></li>
    </ul>
    <p>Today, we address the final question. What happens when we add stochastic noise to the dynamics?</p>

    <h2>Stochasticity in Deep Learning</h2>
    <p>Real-world training is not a clean gradient descent. Noise arises from many sources:</p>
    <ul>
        <li><strong>Stochastic Gradient Descent (SGD):</strong> Gradients are computed on mini-batches of data, which is a noisy estimate of the true gradient.</li>
        <li><strong>Discretization Error:</strong> Using a finite step size $\eta > 0$ introduces error.</li>
        <li><strong>Round-off Errors:</strong> Floating point arithmetic introduces small, random perturbations.</li>
    </ul>
    <p>How do these stochastic effects interact with the Riemannian geometry of the problem? One tool is the <strong>Riemannian Langevin Equation (RLE)</strong>. It can be used to model <strong>noise due to round-off errors</strong></p>

    <h2>The Central Idea</h2>
    <p style="font-size:1.3em; text-align:center; font-style: italic; margin: 2em 0;">Noise in the "upstairs" parameter space (along redundant directions) induces a deterministic, curvature-driven drift in the "downstairs" solution space.</p>
    
    <h2>Warm-Up: Dyson Brownian Motion</h2>
    <p>To build intuition, we study a famous model from random matrix theory.</p>
    <ul>
        <li><strong>Downstairs Dynamics (Eigenvalues):</strong> A system of interacting particles (eigenvalues) $x_1 < \dots < x_d$ evolves according to:
         \[ dx_i = \sum_{j \neq i} \frac{1}{x_i - x_j} dt + \sqrt{\frac{2}{\beta}} dW_i \]
         This is **Dyson Brownian Motion**, combining repulsion and random noise.
        </li>
    </ul>
    
    <h2>Prerequisites for Theorem 15</h2>
    <ul>
        <li>Let $\text{Her}_d$ be the space of $d \times d$ Hermitian matrices.</li>
        <li>Given a vector of eigenvalues $x \in \mathbb{R}^d$, the **isospectral orbit** $O_x$ is the set of all Hermitian matrices with those eigenvalues:
        \[ O_x = \{ M \in \text{Her}_d \,|\, M = UXU^*, U \in U_d \} \]
        where $X = \text{diag}(x)$ and $U_d$ is the unitary group.
        </li>
        <li>We can model noise on the full matrix space ("upstairs") using a standard Wiener process $H_t$ on $\text{Her}_d$. Let $P_M$ and $P_M^\perp$ be projections onto the tangent and normal spaces of the orbit $O_x$ at point $M$.</li>
    </ul>

    <h2>Theorem 15: Noise Upstairs, Curvature Downstairs</h2>
    <p>Consider the "upstairs" SDE for a matrix $M_t$ evolving on the isospectral orbit $O_{x_t}$ subject to isotropic noise:</p>
     \[ dM_t = P_{M_t} dH_t + \sqrt{\frac{2}{\beta}} P_{M_t}^\perp dH_t \]
    <div class="theorem-box">
        <p><strong>Theorem 15 (Huang, Inauen, Menon, informal).</strong></p>
        <ol style="padding-left: 1.5em; margin-top: 1em;">
            <li style="list-style-type: '(a) '; margin-bottom: 0.5em;"><em>The eigenvalues of the matrix $M_t$ have the same law as the solution $x_t$ to Dyson Brownian Motion.</em></li>
            <li style="list-style-type: '(b) ';"><em>In the zero-noise limit ($\beta \to \infty$), noise purely tangential to the orbit ($P_{M_t}dH_t$) induces a deterministic drift normal to the orbit, equal to motion by (minus one half) **mean curvature**.</em></li>
        </ol>
    </div>
    <p>This is a profound result: purely random fluctuations in the "gauge" directions (upstairs) create a deterministic, geometry-driven motion (a drift) in the observable space (downstairs).</p>

    <h2>General Geometric Framework</h2>
    <h3>General Principles 1: Submersion with Group Action</h3>
    <p>We formalize the "upstairs/downstairs" picture.</p>
    <ul>
        <li>Let $(\mathcal{M}, g)$ be a reference Riemannian manifold .</li>
        <li>Let $G$ be a Lie group that acts on $\mathcal{M}$ by isometries .</li>
        <li>The "downstairs" space is the quotient space $M = \mathcal{M}/G$, equipped with a metric $h$ via **Riemannian submersion**.</li>
        <li>The map $\phi: \mathcal{M} \to M$ is the projection. The inverse image $\phi^{-1}(x) = O_x$ is the group orbit over $x$.</li>
    </ul>
    
    <h3>General Principles 2: The "Upstairs" RLE</h3>
    <p>We define stochastic gradient descent via the RLE of a lifted loss function $L = E \circ \phi$.</p>
    <div class="theorem-box">
        <p>Consider the "upstairs" dynamics for $m \in \mathcal{M}$ given by the SDE:</p>
        \[ dm^{\beta,\kappa} = -\text{grad}_g L(m) dt + P_m d\mathbf{M}^\beta + \sqrt{\kappa} P_m^\perp d\mathbf{M}^\beta \]
    </div>
    <ul>
        <li>The first term is the standard gradient of the lifted loss.</li>
        <li>$P_m$ and $P_m^\perp$ are projections onto directions tangential and normal to the group orbit $O_{\phi(m)}$.</li>
        <li>$\mathbf{M}^\beta$ is Brownian motion on $\mathcal{M}$. $\kappa$ modulates the anisotropy of the noise.</li>
    </ul>
    
    <h3>General Principles 3: The "Downstairs" SDE</h3>
    <p>The "upstairs" stochastic flow projects to a stochastic flow downstairs.</p>
    <div class="theorem-box">
        <p>The flow of $m_t$ projects to the RLE for the **free energy** downstairs:</p>
        \[ dx = -\text{grad}_h F_\beta(x) dt + dX^{\beta/\kappa} \]
        <p>where the free energy is defined as:</p>
        \[ F_\beta(x) = L(x) - \frac{1}{\beta}S(x) \quad \text{and} \quad S(x) = \log\text{vol}(O_x) \]
    </div>
    <p>Noise in the redundant "gauge" directions upstairs manifests as an entropic term that modifies the energy landscape downstairs. In the limit $\kappa \to 0$, the upstairs noise is purely tangential, and the downstairs flow becomes deterministic: $\dot{x} = -\text{grad}_h F_\beta(x)$.</p>

    <h2>Application to the Deep Linear Network</h2>
    <h3>RLE for DLN: Upstairs Dynamics</h3>
    <p>Applying the general principle to the DLN, the "upstairs" RLE on the balanced manifold $\mathcal{M}$ is:</p>
    <div class="theorem-box">
        \[ d\mathbf{W}^{\beta,\kappa} = -\nabla_{\mathbf{W}} E(\phi(\mathbf{W})) dt + d\mathbf{M}^{\beta,\kappa} \]
    </div>
    <p>This is standard gradient descent on the lifted loss function $L=E \circ \phi$, plus a noise term $d\mathbf{M}^{\beta,\kappa}$ that represents Brownian motion on the balanced manifold (with the Frobenius metric).</p>

    <h3>RLE for DLN: Downstairs Dynamics</h3>
    <p>The law of the end-to-end matrix $W_t = \phi(\mathbf{W}_t)$ is then given by the "downstairs" RLE:</p>
    <div class="theorem-box">
        \[ dW^{\beta,\kappa} = -\text{grad}_{g^N} F_\beta(W^{\beta,\kappa}) dt + dX^{\beta/\kappa} \]
    </div>
    <ul>
        <li>The drift term is the Riemannian gradient of the free energy $F_\beta(W) = E(W) - \frac{1}{\beta}S(W)$.</li>
        <li>The noise term $dX^{\beta/\kappa}$ is Brownian motion on the downstairs manifold $(M_d, g^N)$.</li>
    </ul>
    
    <h2>The Free Energy Gradient in the DLN</h2>
    <p>The gradient of the free energy, which drives the system's evolution, can be computed explicitly. It balances the drive to minimize loss with an opposing entropic force.</p>
    <div class="theorem-box">
        \[ \text{grad}_{g^N} F_\beta(W) = \underbrace{A_{N,W}(E'(W))}_{\text{Loss Term}} - \underbrace{\frac{1}{\beta}\text{grad}_{g^N}S(W)}_{\text{Entropic/Curvature Term}} \]
    </div>
    <p>The second term is the entropic force arising from the geometry of overparameterization. For the DLN, it takes the specific form:</p>
    <div class="theorem-box">
        \[ \frac{1}{\beta}\text{grad}_{g^N}S(W) = \frac{1}{\beta} Q_N \Sigma' Q_0^T \]
    </div>
    <ul>
        <li>
            <b>\( W = Q_N \Sigma Q_0^T \):</b> This is the Singular Value Decomposition (SVD) of the end-to-end matrix. \(Q_N\) and \(Q_0\) are orthogonal matrices, and \(\Sigma\) is a diagonal matrix containing the singular values \( \sigma_k \).
        </li>
        <li>
            <b>\( \Sigma' \):</b> A diagonal matrix whose entries are functions of the singular values \( \sigma_k \) and network depth \( N \). It acts as a repulsive force between the singular values, pushing the system away from low-rank solutions. The paper notes this "physical effect" is strictly due to the geometry of the DLN.
        </li>
    </ul>
    
    <h2>The Explicit Downstairs SDE</h2>
    <p>The Menon & Yu paper provides an explicit formula for the Brownian motion $dX^{\beta/\kappa}$ on $(M_d, g^N)$.</p>
    <div class="theorem-box">
        <p><strong>Theorem 18 (Menon & Yu, 2023).</strong> The solution $X_t^\beta$ to the following ItÃ´ SDE with initial condition $X_0=W_0$ is Brownian motion on $(M_d, g^N)$:</p>
        \[ dX_t^\beta = \sqrt{\frac{2}{\beta}} \begin{pmatrix} \sqrt{N}\lambda_1^{N-1}dB_{11}^{1,1} & \dots \\ \vdots & \ddots \end{pmatrix} + \frac{1}{\beta} Q_N \Sigma'' Q_0^T dt \]
    </div>
    <p>Here, $\Lambda = \Sigma^{1/N}$, $dB$ is a matrix of standard Wiener processes, and $\Sigma''$ is a diagonal matrix of drift terms (related to mean curvature) that arises from the ItÃ´ correction.</p>

    <h2>Proof Sketch for Theorem 15 (2x2 Case)</h2>
    <h3>The Goal</h3>
    <p>Let's prove Theorem 15 for the simple case of $d=2$. We want to show that the eigenvalues $x_1, x_2$ of the matrix $M_t$ evolving by:</p>
    \[ dM_t = P_{M_t} dH_t + \sqrt{\frac{2}{\beta}} P_{M_t}^\perp dH_t \]
    <p>follow the Dyson Brownian Motion equations:</p>
    \[ dx_1 = \frac{1}{x_1 - x_2} dt + \sqrt{\frac{2}{\beta}} dW_1 \]
    \[ dx_2 = \frac{1}{x_2 - x_1} dt + \sqrt{\frac{2}{\beta}} dW_2 \]
    
    <h3>Step 1: Simplification</h3>
    <p>The dynamics are invariant under unitary transformations ($M \to UMU^*$). This allows us to analyze the process at a point where the matrix $M_t$ is diagonal, without loss of generality.</p>
    <ul>
        <li>Let's fix a time $t$ and assume $M_t$ is the diagonal matrix $X = \text{diag}(x_1, x_2)$.</li>
        <li><strong>Normal Space ($T_X^\perp O_x$):</strong> The normal space to the orbit at $X$ consists of all 2x2 Hermitian matrices that commute with $X$. These are the diagonal matrices.
            \[ T_X^\perp O_x = \left\{ \begin{pmatrix} a & 0 \\ 0 & b \end{pmatrix}, a, b \in \mathbb{R} \right\} \]
        </li>
        <li><strong>Tangent Space ($T_X O_x$):</strong> The tangent space is the orthogonal complement. It consists of all 2x2 Hermitian matrices with zeros on the diagonal.
            \[ T_X O_x = \left\{ \begin{pmatrix} 0 & z \\ \bar{z} & 0 \end{pmatrix}, z \in \mathbb{C} \right\} \]
        </li>
    </ul>

    <h3>Step 2: Decomposing the Noise</h3>
    <p>We express the "upstairs" noise $dH_t$ using an orthonormal basis for 2x2 Hermitian matrices that respects our tangent/normal split.</p>
    <p><b>Normal Basis:</b> $E_a = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$, $E_b = \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}$</p>
    <p><b>Tangent Basis:</b> $E_c = \frac{1}{\sqrt{2}}\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$, $E_d = \frac{1}{\sqrt{2}}\begin{pmatrix} 0 & i \\ -i & 0 \end{pmatrix}$</p>
    <p>The matrix noise term $dH_t$ can be written with four independent Wiener processes $W_a, W_b, W_c, W_d$:</p>
    \[ dH_t = E_a dW_a + E_b dW_b + E_c dW_c + E_d dW_d \]
    <p>Projecting this noise onto the tangent ($P_X$) and normal ($P_X^\perp$) spaces at $X$ gives our SDE for $dM_t$:</p>
    \[ dM_t = \underbrace{(E_c dW_c + E_d dW_d)}_{\text{Tangent Part}} + \sqrt{\frac{2}{\beta}} \underbrace{(E_a dW_a + E_b dW_b)}_{\text{Normal Part}} \]

    <h3>Step 3: ItÃ´'s Formula for Eigenvalues</h3>
    <p>To find the dynamics of an eigenvalue, say $x_1(M_t)$, we use ItÃ´'s formula. This requires the first and second derivatives of the eigenvalue with respect to changes in the matrix.</p>
    <p>For a function $f(M_t)$, ItÃ´'s formula is: $df = \text{D}f(dM_t) + \frac{1}{2}\text{D}^2f(dM_t, dM_t)$</p>
    <ul>
        <li><strong>First Derivative (captures noise):</strong> The change in an eigenvalue is most sensitive to the corresponding diagonal entry.
            \[ \text{D}x_1(A) = A_{11} \]
        </li>
        <li><strong>Second Derivative (captures drift):</strong> The second-order change depends on interactions between off-diagonal elements. For a matrix $A$:
             \[ \text{D}^2x_1(A, A) = 2 \frac{|A_{12}|^2}{x_1 - x_2} \]
        </li>
    </ul>
    <p>Now we just need to plug our $dM_t$ into these formulas.</p>

    <h3>Step 4: The Final Calculation</h3>
    <p>Let's compute the terms for $dx_1$.</p>
    <ul>
        <li><strong>Noise Term:</strong> We apply the first derivative to $dM_t$. Only the normal part contributes to the diagonal.
        \[ \text{D}x_1(dM_t) = (dM_t)_{11} = \left(\sqrt{\frac{2}{\beta}} (E_a dW_a + E_b dW_b)\right)_{11} = \sqrt{\frac{2}{\beta}} dW_a \]
        This gives the Brownian motion part of the equation. Let's call $W_a$ our new $W_1$.
        </li>
        <li><strong>Drift Term:</strong> We apply the second derivative. Only the tangent part has off-diagonal entries. The quadratic variation of $dM_t$ in the off-diagonal is $[E_c,E_c]dt + [E_d,E_d]dt$.
        \[ \frac{1}{2} \text{D}^2x_1(dM_t, dM_t) = \frac{1}{2} \left( \underbrace{\text{D}^2x_1(E_c,E_c)dt}_{\text{from }dW_c^2} + \underbrace{\text{D}^2x_1(E_d,E_d)dt}_{\text{from }dW_d^2} \right) \]
        \[ = \frac{1}{2} \left( 2\frac{|(E_c)_{12}|^2}{x_1 - x_2} + 2\frac{|(E_d)_{12}|^2}{x_1 - x_2} \right) dt = \frac{1}{x_1 - x_2} \left( \left|\frac{1}{\sqrt{2}}\right|^2 + \left|\frac{i}{\sqrt{2}}\right|^2 \right) dt = \frac{1}{x_1 - x_2} dt \]
        </li>
    </ul>
    <p>Combining these gives: $dx_1 = \frac{1}{x_1 - x_2} dt + \sqrt{\frac{2}{\beta}} dW_1$. The proof for $x_2$ is identical.</p>
    
    <h2>Summary of Findings</h2>
    <ul>
        <li>Noise in training can be modeled rigorously using the <strong>Riemannian Langevin Equation</strong>.</li>
        <li>There is a deep connection, via Riemannian submersion, between stochastic dynamics in the "upstairs" parameter space and the resulting dynamics in the "downstairs" solution space.</li>
        <li>Noise that is tangential to the fibers (group orbits) upstairs induces a deterministic drift downstairs related to the <strong>mean curvature</strong> of the fibers. This drift is equivalent to the gradient of the <strong>Boltzmann entropy</strong> (volume).</li>
        <li>For the DLN, we have explicit formulas for this stochastic process, which corresponds to gradient descent of a <strong>free energy</strong> functional, combining the original loss with an entropic term that favors high-volume, low-rank solutions.</li>
    </ul>

    <div style="height: 60px;"></div>
</div>
</body>
</html>