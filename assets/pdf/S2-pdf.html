<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implicit Acceleration in Deep Linear Networks</title>
    <!-- MathJax 3 for rendering LaTeX math -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                macros: {
                  Tr: '\\operatorname{Tr}',
                  grad: '\\operatorname{grad}',
                  Pr: '\\operatorname{Pr}'
                },
                packages: {'[+]': ['physics', 'ams']}
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            margin: 0 auto;
            padding: 0 0 40px 0;
            background-color: #f0f0f0;
            color: #333;
            line-height: 1.7;
            max-width: 900px;
        }
        .pdf-container {
            background: #fff;
            margin: 40px auto 0 auto;
            padding: 40px 60px;
            border-radius: 10px;
            box-shadow: 0 4px 24px rgba(0,0,0,0.10);
        }
        h1, h2, h3, h4 {
            color: #0056b3;
            margin-top: 2.2em;
            margin-bottom: 0.7em;
            font-weight: 600;
        }
        h1 { font-size: 2.3em; margin-top: 0.5em; text-align:center; }
        h2 { font-size: 1.6em; border-bottom: 2px solid #a0aec0; padding-bottom: 0.2em; }
        h3 { font-size: 1.2em; color: #444; }
        h4 { font-size: 1.1em; color: #0056b3; }
        p, ul, ol {
            font-size: 1.13em;
            margin-bottom: 1.2em;
        }
        ul, ol { padding-left: 2em; }
        ul li { list-style-type: disc; margin-bottom: 10px; }
        .theorem-box {
            border: 2px solid #0056b3;
            border-radius: 8px;
            padding: 20px 30px;
            margin: 30px 0;
            background-color: #f7faff;
            box-shadow: 0 2px 8px rgba(0, 86, 179, 0.08);
            text-align: center;
        }
        .theorem-box p { text-align: left; font-size: 1.1em; font-weight: 500; }
        .image-placeholder {
            border: 1px solid #ccc;
            border-radius: 8px;
            padding: 20px;
            text-align: center;
            background-color: #f9f9f9;
            margin: 10px;
            flex: 1;
        }
        .image-placeholder-container {
            display: flex;
            justify-content: center;
            gap: 30px;
            width:100%;
        }
        @media (max-width: 900px) {
            .pdf-container { padding: 18px 8px; }
            .columns { flex-direction: column; gap: 0; }
            .image-placeholder-container { flex-direction: column; }
        }
    </style>
</head>
<body>
<div class="pdf-container">
    <h1>Deep Linear Networks</h1>
    <h3 style="text-align:center; color:#444; margin-top:-1.2em;">Rathindra Nath Karmakar</h3>
    <h3 style="text-align:center; color:#444; margin-top:-0.5em;">Session 2 : Implicit Acceleration</h3>

    <h2>References</h2>
    <ul>
        <li><b>On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization</b>, Sanjeev Arora, Nadav Cohen, Elad Hazan (2018)</li>
        <li><b>The geometry of the deep linear network</b>, Govind Menon (2024)</li>
    </ul>

    <h2>Recap: Key Questions for Gradient Flow</h2>
    <p>For the gradient flow dynamics on a loss surface $\mathcal{L}(\mathbf{W})$:</p>
    <div class="theorem-box" style="margin: 20px 0;">
        \[ \frac{d}{dt} \mathbf{W}(t) = - \nabla_{\mathbf{W}} \mathcal{L}(\mathbf{W}(t)) \]
    </div>
    <p>We want to understand:</p>
    <ul>
        <li>Convergence guarantees? (<em>Yes, for balanced cases</em>)</li>
        <li><strong>Convergence rate?</strong></li>
        <li>Characterization of the minimizer reached?</li>
        <li>Effect of noise and discretization?</li>
    </ul>
    <p>Today, we address the second question: can overparameterization improve the <strong>rate of convergence</strong>?</p>

    <h2>The Counter-Intuitive Message</h2>
    <p>The conventional wisdom is that increasing depth improves expressiveness but complicates optimization. Today's message, based on Arora, Cohen, & Hazan (2018), is that sometimes:</p>
    <div class="theorem-box">
        <p style="font-size:1.4em; text-align:center; color:#0056b3;">Increasing depth can <em>accelerate</em> optimization.</p>
    </div>
    <p>We will see that overparameterization via depth can act as an implicit preconditioner, combining the effects of momentum and adaptive learning rates.</p>

    <h2>Empirical Evidence: Acceleration with Depth</h2>
    <p>Comparison of gradient descent on a linear regression task with $\ell_p$ loss. (Fig 3 from Arora et al. 2018)</p>
    <div class="image-placeholder-container">
        <div class="image-placeholder">
            <h4>L2 Loss Convergence</h4>
            <p>(Image showing training loss vs. iteration. Curves for depth 1, 2, 4, 8 show that deeper networks converge slightly slower than the shallow network.)</p>
        </div>
        <div class="image-placeholder">
            <h4>L4 Loss Convergence</h4>
            <p>(Image showing training loss vs. iteration. Curves for depth 1, 2, 4, 8 show that deeper networks converge dramatically faster than the shallow network.)</p>
        </div>
    </div>
    <ul>
        <li><strong>Left ($\ell_2$ loss):</strong> Increasing depth slightly *slows down* convergence, consistent with prior work.</li>
        <li><strong>Right ($\ell_4$ loss):</strong> Increasing depth provides a **massive acceleration**, outperforming the shallow model significantly.</li>
    </ul>

    <h2>The End-to-End Update Rule</h2>
    <p>Let $W_e = W_N W_{N-1} \cdots W_1$ be the end-to-end matrix. The paper shows that its gradient flow dynamics can be written purely in terms of $W_e$ itself.</p>
    <div class="theorem-box">
        <p><strong>Theorem 1 (Arora et al. 2018).</strong> Under the balanced initialization assumption, $W_{j+1}(t_0)^T W_{j+1}(t_0) = W_j(t_0)W_j(t_0)^T$, the gradient flow for $W_e$ is:</p>
        \[ \dot{W}_e(t) = - \eta \sum_{j=1}^{N} \left[ W_e(t) W_e(t)^T \right]^{\frac{N-j}{N}} \cdot \nabla\mathcal{L}(W_e(t)) \cdot \left[ W_e(t)^T W_e(t) \right]^{\frac{j-1}{N}} \]
    </div>
    <p>The standard gradient $\nabla\mathcal{L}(W_e)$ is pre-multiplied and post-multiplied by fractional matrix powers of $W_e W_e^T$ and $W_e^T W_e$.</p>

    <h2>A Geometric Interpretation</h2>
    <p>The complex update rule from Theorem 1 seems arbitrary. But it has a beautiful geometric meaning. Let's define the linear preconditioning operator from Theorem 1 as $A_{N,W_e}$:</p>
    \[ A_{N,W_e}(Z) = \sum_{k=1}^{N} (W_e W_e^T)^{\frac{N-k}{N}} Z (W_e^T W_e)^{\frac{k-1}{N}} \]
    <p>So the dynamics are $\dot{W}_e = -A_{N,W_e} (E'(W_e))$, where $E'(W_e) = \nabla\mathcal{L}(W_e)$ is the standard Euclidean gradient.</p>
    <p>Following Menon (Sec 4.3), we define a position-dependent Riemannian metric $g^N$ on the tangent space at $W_e$ via the inner product:</p>
    <div class="theorem-box">
         <p><strong>Definition: A New Metric.</strong></p>
         \[ g^N(W_e)(Z_1, Z_2) := \text{Tr}\left(Z_1^T (A_{N,W_e})^{-1} Z_2\right) \]
         <p style="text-align:left; font-size:1em; margin-top:10px;">(Assuming $W_e$ has full rank, $A_{N,W_e}$ is invertible.)</p>
    </div>
    <p>Under this specific metric, the complicated dynamics of Theorem 1 become a simple and natural <strong>Riemannian gradient flow</strong>:</p>
    <div class="theorem-box" style="background-color: #e6f7ff; border-color: #0056b3;">
        \[ \dot{W}_e(t) = -\text{grad}_{g^N} E(W_e(t)) \]
    </div>

    <h2>Interpreting the Dynamics (Single Output Case)</h2>
    <p>For the special case of a single output ($k=1$), the preconditioning scheme simplifies to a more intuitive form.</p>
    <div class="theorem-box">
    <p><strong>Claim 2 (Arora et al. 2018).</strong> For a single output network, the end-to-end dynamics are equivalent to:</p>
    \[ \dot{W}_e = -\eta \underbrace{\|W_e\|^{2 - \frac{2}{N}}}_{\text{Adaptive LR}} \left( \nabla\mathcal{L}(W_e) + \underbrace{(N-1)\Pr_{W_e}(\nabla\mathcal{L}(W_e))}_{\text{Momentum-like term}} \right) \]
     <p style="margin-top:10px;">where $\Pr_{W_e}(V)$ is the orthogonal projection of vector $V$ onto the direction of vector $W_e$.</p>
    </div>
    <ul>
        <li><strong>Adaptive Learning Rate:</strong> As the weight vector $\|W_e\|$ grows (moves away from zero init), the effective learning rate increases.</li>
        <li><strong>Momentum:</strong> The gradient is amplified along the direction of the current weight vector $W_e$. Since $W_e$ is the integral of past updates, this promotes movement along the historical trajectory.</li>
    </ul>

    <h1>Detailed Derivations</h1>

    <h2>Proof of Theorem 1 (N=2 case)</h2>
    <p>Let's derive the end-to-end dynamics for $W_e = W_2 W_1$. The time derivative is $\dot{W}_e = \dot{W}_2 W_1 + W_2 \dot{W}_1$.</p>
    <ol>
        <li>Substitute the gradient flow dynamics for each layer:
            \[ \dot{W}_1 = -\eta W_2^T \nabla\mathcal{L}(W_e) \quad , \quad \dot{W}_2 = -\eta \nabla\mathcal{L}(W_e) W_1^T \]
        </li>
        <li>Plug these into the expression for $\dot{W}_e$:
            \[ \dot{W}_e = (-\eta \nabla\mathcal{L}(W_e) W_1^T) W_1 + W_2 (-\eta W_2^T \nabla\mathcal{L}(W_e)) \]
        </li>
        <li>Rearrange the terms:
            \[ \dot{W}_e = -\eta \left( (W_2 W_2^T) \nabla\mathcal{L}(W_e) + \nabla\mathcal{L}(W_e) (W_1^T W_1) \right) \]
        </li>
        <li>Using the identities for the N=2 case, $W_2 W_2^T = (W_e W_e^T)^{1/2}$ and $W_1^T W_1 = (W_e^T W_e)^{1/2}$:
            <div class="theorem-box" style="margin-top:20px;">
            \[ \dot{W}_e = -\eta \left( (W_e W_e^T)^{1/2} \nabla\mathcal{L}(W_e) + \nabla\mathcal{L}(W_e) (W_e^T W_e)^{1/2} \right) \]
            </div>
            This matches the general formula from Theorem 1 for $N=2$.
        </li>
    </ol>
    
    <h2>Deriving the Preconditioner</h2>
    <h3>SVD Setup (N=3 case)</h3>
    <p>Let's analyze $N=3$. Let the SVD of each layer be $W_j = U_j \Sigma_j V_j^T$.</p>
    <ul>
        <li><strong>Balance Invariants:</strong> $W_2^T W_2 = W_1 W_1^T$. Hence, their spectra are equal as well $\implies$ $W_1$ and $W_2$ have the same set of singular values.</li>
        <li>In terms of Polar decomposition, the balance equation implies $P_2 = W_2^T W_2 = W_1W_1^T = R_1$. Hence, $V_2$ and $U_1$ can be chosen to be equal. Therefore, we get the SVD decompositions: $W_2 = U_2 \Sigma V_2^T$ and $W_1 = V_2 \Sigma V_1^T$.</li>
        <li>Similarly using the balance equation for $W_2$ and $W_3$, we get the SVDs: $W_3 = U_3 \Sigma V_3^T$, $W_2 = V_3 \Sigma V_2^T$ and $W_1 = V_2 \Sigma V_1^T$.</li>
    </ul>

    <h3>Simplifying the Product</h3>
    <p>Let's express the end-to-end matrix $W_e = W_3 W_2 W_1$ using the SVDs and the relationships we found. The expression simplifies because the intermediate orthogonal matrices cancel out:</p>
    \[ W_e = U_3 \Sigma^3 V_1^T \]
    <p>This looks like an SVD for $W_e$, with singular value matrix $\Sigma^3$ and orthogonal matrices $U_3$ and $V_1^T$.</p>

    <h3>Final Identities</h3>
    <p>From $W_e = U_3 \Sigma^3 V_1^T$, we can identify the SVD components of $W_e = U_e \Sigma_e V_e^T$ as $U_e = U_3$, $\Sigma_e = \Sigma^3$, $V_e = V_1$.</p>
    <p>Now we can express the individual layer terms using the end-to-end SVD components.</p>
    <ul>
        <li>For the last layer, $W_3 W_3^T = U_3 \Sigma^2 U_3^T$. Since $U_3 = U_e$ and $\Sigma = \Sigma_e^{1/3}$:
            \[ W_3 W_3^T = U_e (\Sigma_e^{1/3})^2 U_e^T = (U_e \Sigma_e U_e^T)^{2/3} = (W_e W_e^T)^{2/3} \]
        </li>
         <li>For the first layer, $W_1^T W_1 = V_1 \Sigma^2 V_1^T$. Using $\Sigma = \Sigma_e^{1/3}$ and $V_1 = V_e$:
            \[ W_1^T W_1 = V_1 (\Sigma_e^{1/3})^2 V_1^T = (V_e^T \Sigma_e V_e)^{2/3} = (W_e^T W_e)^{2/3} \]
        </li>
    </ul>

    <h2>How This Leads to Acceleration</h2>
    <h3>Setup</h3>
    <p>We analyze a simple, ill-conditioned linear regression problem with $\ell_4$ loss and $N=2$ overparameterization.</p>
    <ul>
        <li><strong>Data:</strong> Two points, $([1,0], y_1)$ and $([0,1], y_2)$.</li>
        <li><strong>Loss:</strong> $L(w_1, w_2) = \frac{1}{4}(w_1 - y_1)^4 + \frac{1}{4}(w_2 - y_2)^4$.</li>
        <li><strong>Ill-Conditioning:</strong> Assume $|y_1| \gg |y_2| \approx 1$.</li>
        <li><strong>Initialization:</strong> Near-zero weights, $w_1^{(0)}=\epsilon_1, w_2^{(0)}=\epsilon_2$, with $\epsilon_2/\epsilon_1 \approx y_2/y_1$.</li>
    </ul>
    
    <h3>Standard GD</h3>
    <p>The standard gradient descent update for each coordinate is decoupled:</p>
    \[ w_i^{(t+1)} \leftarrow w_i^{(t)} - \eta (w_i^{(t)} - y_i)^3 \]
    <ul>
        <li>To avoid divergence, the learning rate $\eta$ is limited by the coordinate with the largest gradient, which is $w_1$. The stability condition requires: $\eta < \frac{2}{(w_1-y_1)^2} \approx \frac{2}{y_1^2}$.</li>
        <li>This very small learning rate, dictated by $y_1$, is then applied to the update for $w_2$.</li>
        <li>The convergence for $w_2$ is extremely slow. The error $\Delta_2 = w_2 - y_2$ shrinks by a factor of approximately $(1 - \eta y_2^2)$ at each step, which is very close to 1.</li>
    </ul>

    <h3>Overparameterized GD</h3>
    <p>For $N=2$, the discrete update rule for a single output network is:</p>
    \[ \mathbf{w}^{(t+1)} \leftarrow \mathbf{w}^{(t)} - \eta \left( \|\mathbf{w}^{(t)}\| \nabla\mathcal{L}(\mathbf{w}^{(t)}) + \Pr_{\mathbf{w}^{(t)}}(\nabla\mathcal{L}(\mathbf{w}^{(t)})) \right) \]
    <p>Using the condition $\epsilon_2/\epsilon_1 \approx y_2/y_1 \ll 1$, we have $\|\mathbf{w}^{(0)}\| \approx \epsilon_1$. The updates simplify to:</p>
    \[ w_1^{(1)} \approx \epsilon_1 + \eta(2\epsilon_1 y_1^3) \quad , \quad w_2^{(1)} \approx \epsilon_2 + \eta(\epsilon_1 y_2^3 + \epsilon_2 y_1^3) \]
    
    <h3>The Punchline</h3>
    <ol>
        <li>
            <strong>Choose $\eta$:</strong> We can now choose a large learning rate for $w_1$ to make it converge in one step. Let $\eta = \frac{1}{2\epsilon_1 y_1^2}$. This gives $w_1^{(1)} \approx y_1$.
        </li>
        <li>
            <strong>Analyze $w_2$ update:</strong> With this $\eta$, the update for $w_2$ becomes approximately $\frac{y_2}{2}$.
        </li>
        <li>
            <strong>Compare Rates:</strong> In one step, $w_2$ has moved halfway to its target $y_2$. The effective learning rate for $w_2$ after $w_1$ converges is $\eta_{OP} \approx \frac{1}{2\epsilon_1 y_1}$. The speedup is:
            \[ \frac{\eta_{OP}}{\eta_{GD}} > \frac{1/(2\epsilon_1 y_1)}{2/y_1^2} = \frac{y_1}{4\epsilon_1} \gg 1 \]
        </li>
    </ol>
    <p>Overparameterization allows the large coordinate to "lend" its scale to accelerate the small coordinate.</p>

    <h2>Next Time...</h2>
    <p style="font-size: 1.3em; text-align:center;">Characterizing the Minimizer</p>
    
    <h2 style="margin-top: 3em;">Thank You!</h2>
    <p style="font-size: 1.3em; text-align:center;">Any Questions?</p>

    <div style="height: 60px;"></div>
</div>
</body>
</html>
